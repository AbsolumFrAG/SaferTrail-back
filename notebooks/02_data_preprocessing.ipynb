{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f808c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Début du preprocessing des données\n",
      "============================================================\n",
      "\n",
      "📁 Chargement des datasets...\n",
      "\n",
      "📊 État du chargement:\n",
      "  311 Reports: ✅ Chargé\n",
      "  Crime Incidents: ✅ Chargé\n",
      "  Vision Zero Crashes: ✅ Chargé\n",
      "\n",
      "============================================================\n",
      "🧹 NETTOYAGE DU DATASET 311\n",
      "============================================================\n",
      "📋 Dataset initial: 709 lignes\n",
      "\n",
      "🕐 Nettoyage des dates...\n",
      "   - Lignes supprimées (dates invalides): 0\n",
      "\n",
      "🗺️ Nettoyage des coordonnées...\n",
      "   - Lignes supprimées (coordonnées invalides): 2\n",
      "\n",
      "📝 Nettoyage des champs textuels...\n",
      "\n",
      "🎯 Classification des priorités...\n",
      "✅ Dataset 311 nettoyé: 707 lignes\n",
      "   Répartition des priorités:\n",
      "     - MEDIUM: 707 (100.0%)\n",
      "\n",
      "============================================================\n",
      "🚨 NETTOYAGE DU DATASET CRIME\n",
      "============================================================\n",
      "📋 Dataset initial: 185,534 lignes\n",
      "\n",
      "🕐 Nettoyage des dates...\n",
      "   - Lignes supprimées (dates invalides): 0\n",
      "\n",
      "🗺️ Nettoyage des coordonnées...\n",
      "   - Lignes supprimées (coordonnées invalides): 11,468\n",
      "\n",
      "⚖️ Classification de la gravité...\n",
      "✅ Dataset Crime nettoyé: 174,066 lignes\n",
      "   Répartition par gravité:\n",
      "     - LOW: 172,803 (99.3%)\n",
      "     - HIGH: 1,263 (0.7%)\n",
      "\n",
      "============================================================\n",
      "🚗 NETTOYAGE DU DATASET VISION ZERO\n",
      "============================================================\n",
      "📋 Dataset initial: 37,896 lignes\n",
      "\n",
      "🕐 Nettoyage des dates...\n",
      "   - Lignes supprimées (dates invalides): 0\n",
      "\n",
      "🗺️ Nettoyage des coordonnées...\n",
      "   - Lignes supprimées (coordonnées invalides): 0\n",
      "\n",
      "🚦 Standardisation des modes de transport...\n",
      "\n",
      "⚖️ Classification de la gravité...\n",
      "✅ Dataset Crashes nettoyé: 37,896 lignes\n",
      "   Répartition par mode de transport:\n",
      "     - MOTOR_VEHICLE: 28,173 (74.3%)\n",
      "     - PEDESTRIAN: 6,083 (16.1%)\n",
      "     - BICYCLE: 3,640 (9.6%)\n",
      "\n",
      "============================================================\n",
      "🔗 HARMONISATION DES DATASETS\n",
      "============================================================\n",
      "\n",
      "⏰ Standardisation des colonnes temporelles...\n",
      "\n",
      "🗺️ Standardisation des coordonnées...\n",
      "✅ Coordonnées standardisées pour tous les datasets\n",
      "\n",
      "🔄 Création d'un dataset unifié...\n",
      "✅ Dataset unifié créé: 212,669 incidents\n",
      "\n",
      "📊 Statistiques du dataset unifié:\n",
      "  - CRIME_INCIDENTS: 174,066 (81.8%)\n",
      "  - TRAFFIC_CRASHES: 37,896 (17.8%)\n",
      "  - 311_REPORTS: 707 (0.3%)\n",
      "\n",
      "  Répartition par gravité globale:\n",
      "    - LOW: 187,861 (88.3%)\n",
      "    - MEDIUM: 18,788 (8.8%)\n",
      "    - HIGH: 6,020 (2.8%)\n",
      "\n",
      "============================================================\n",
      "✅ VALIDATION DE LA QUALITÉ DES DONNÉES\n",
      "============================================================\n",
      "\n",
      "🔍 Validation pour 311:\n",
      "  ✅ latitude: 0 manquantes (0.0%)\n",
      "  ✅ longitude: 0 manquantes (0.0%)\n",
      "  ✅ incident_datetime: 0 manquantes (0.0%)\n",
      "  📅 Période couverte: 137 jours\n",
      "  ✅ Aucune date future détectée\n",
      "  ✅ Toutes les coordonnées dans les limites de Boston\n",
      "\n",
      "🔍 Validation pour CRIME:\n",
      "  ✅ latitude: 0 manquantes (0.0%)\n",
      "  ✅ longitude: 0 manquantes (0.0%)\n",
      "  ✅ incident_datetime: 0 manquantes (0.0%)\n",
      "  📅 Période couverte: 868 jours\n",
      "  ✅ Aucune date future détectée\n",
      "  ✅ Toutes les coordonnées dans les limites de Boston\n",
      "\n",
      "🔍 Validation pour CRASHES:\n",
      "  ✅ latitude: 0 manquantes (0.0%)\n",
      "  ✅ longitude: 0 manquantes (0.0%)\n",
      "  ✅ incident_datetime: 0 manquantes (0.0%)\n",
      "  📅 Période couverte: 3652 jours\n",
      "  ✅ Aucune date future détectée\n",
      "  ✅ Toutes les coordonnées dans les limites de Boston\n",
      "\n",
      "🔍 Validation pour DATASET UNIFIÉ:\n",
      "  ✅ latitude: 0 manquantes (0.0%)\n",
      "  ✅ longitude: 0 manquantes (0.0%)\n",
      "  ✅ incident_datetime: 0 manquantes (0.0%)\n",
      "  ⚠️ Erreur calcul période: Cannot compare tz-naive and tz-aware timestamps\n",
      "  ⚠️ Impossible de vérifier les dates futures: Cannot compare tz-naive and tz-aware timestamps\n",
      "  ✅ Toutes les coordonnées dans les limites de Boston\n",
      "\n",
      "============================================================\n",
      "💾 SAUVEGARDE DES DONNÉES NETTOYÉES\n",
      "============================================================\n",
      "✅ Sauvegardé: ../data/processed/cleaned_311.parquet (707 lignes) [Parquet]\n",
      "❌ Erreur sauvegarde Parquet cleaned_crime: (\"Could not convert '232044120' with type str: tried to convert to int64\", 'Conversion failed for column INCIDENT_NUMBER with type object')\n",
      "✅ Sauvegardé: ../data/processed/cleaned_crime.csv (174,066 lignes) [CSV - Fallback]\n",
      "✅ Sauvegardé: ../data/processed/cleaned_crashes.parquet (37,896 lignes) [Parquet]\n",
      "✅ Sauvegardé: ../data/processed/unified_incidents.parquet (212,669 lignes) [Parquet]\n",
      "\n",
      "💡 CONSEIL:\n",
      "   Pour des performances optimales, installez pyarrow:\n",
      "   pip install pyarrow\n",
      "   Cela permettra l'utilisation du format Parquet (plus rapide et compact)\n",
      "✅ Métadonnées sauvegardées: ../data/processed/preprocessing_metadata.json\n",
      "\n",
      "============================================================\n",
      "📋 RÉSUMÉ DU PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "🎯 DATASETS TRAITÉS:\n",
      "  • 311: 707 incidents\n",
      "  • CRIME: 174,066 incidents\n",
      "  • CRASHES: 37,896 incidents\n",
      "\n",
      "📊 DATASET UNIFIÉ: 212,669 incidents totaux\n",
      "  • Période: Erreur calcul dates - Cannot compare tz-naive and tz-aware timestamps\n",
      "  • Couverture géographique: 212669 incidents avec coordonnées\n",
      "\n",
      "💾 FICHIERS SAUVEGARDÉS:\n",
      "  • ../data/processed/cleaned_311.parquet\n",
      "  • ../data/processed/cleaned_crime.csv\n",
      "  • ../data/processed/cleaned_crashes.parquet\n",
      "  • ../data/processed/unified_incidents.parquet\n",
      "\n",
      "✅ QUALITÉ DES DONNÉES:\n",
      "  • Coordonnées valides: 100.0%\n",
      "  • Période temporelle: Error: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True, at position 707\n",
      "\n",
      "🚀 PROCHAINE ÉTAPE:\n",
      "  Les données sont prêtes pour le feature engineering!\n",
      "  Prochain notebook: 03_feature_engineering.ipynb\n",
      "\n",
      "============================================================\n",
      "✨ PREPROCESSING TERMINÉ AVEC SUCCÈS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing des Données - Modèle de Prédiction de Rues Risquées\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Utilitaires pour le nettoyage géospatial\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import folium\n",
    "\n",
    "print(\"🔧 Début du preprocessing des données\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# =====================================================================\n",
    "# 1. CHARGEMENT DES DONNÉES BRUTES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n📁 Chargement des datasets...\")\n",
    "\n",
    "def load_csv_file(filename):\n",
    "    try:\n",
    "        return pd.read_csv(filename)\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            return pd.read_csv(f'../data/raw/{filename}')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️ Fichier non trouvé: {filename}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du chargement de {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Chargement des datasets\n",
    "df_311_raw = load_csv_file('311.csv')\n",
    "df_crime_raw = load_csv_file('crime-incidents-report.csv')\n",
    "df_crashes_raw = load_csv_file('vision-zero-crash-records.csv')\n",
    "\n",
    "# Vérification du chargement\n",
    "datasets_loaded = {\n",
    "    '311 Reports': df_311_raw is not None,\n",
    "    'Crime Incidents': df_crime_raw is not None,\n",
    "    'Vision Zero Crashes': df_crashes_raw is not None\n",
    "}\n",
    "\n",
    "print(\"\\n📊 État du chargement:\")\n",
    "for name, loaded in datasets_loaded.items():\n",
    "    status = \"✅ Chargé\" if loaded else \"❌ Échec\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "\n",
    "# =====================================================================\n",
    "# 2. NETTOYAGE DU DATASET 311\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🧹 NETTOYAGE DU DATASET 311\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_311_raw is not None:\n",
    "    df_311_clean = df_311_raw.copy()\n",
    "    print(f\"📋 Dataset initial: {len(df_311_clean):,} lignes\")\n",
    "    \n",
    "    # ===== Nettoyage des dates =====\n",
    "    print(\"\\n🕐 Nettoyage des dates...\")\n",
    "    \n",
    "    # Conversion des colonnes de date\n",
    "    date_columns = ['open_dt', 'sla_target_dt', 'closed_dt']\n",
    "    for col in date_columns:\n",
    "        if col in df_311_clean.columns:\n",
    "            df_311_clean[col] = pd.to_datetime(df_311_clean[col], errors='coerce')\n",
    "    \n",
    "    # Suppression des lignes avec des dates d'ouverture invalides\n",
    "    before_count = len(df_311_clean)\n",
    "    df_311_clean = df_311_clean.dropna(subset=['open_dt'])\n",
    "    after_count = len(df_311_clean)\n",
    "    print(f\"   - Lignes supprimées (dates invalides): {before_count - after_count:,}\")\n",
    "    \n",
    "    # Ajout de features temporelles dérivées\n",
    "    df_311_clean['open_year'] = df_311_clean['open_dt'].dt.year\n",
    "    df_311_clean['open_month'] = df_311_clean['open_dt'].dt.month\n",
    "    df_311_clean['open_day'] = df_311_clean['open_dt'].dt.day\n",
    "    df_311_clean['open_weekday'] = df_311_clean['open_dt'].dt.dayofweek\n",
    "    df_311_clean['open_hour'] = df_311_clean['open_dt'].dt.hour\n",
    "    \n",
    "    # ===== Nettoyage des coordonnées géographiques =====\n",
    "    print(\"\\n🗺️ Nettoyage des coordonnées...\")\n",
    "    \n",
    "    # Conversion en numérique\n",
    "    df_311_clean['latitude'] = pd.to_numeric(df_311_clean['latitude'], errors='coerce')\n",
    "    df_311_clean['longitude'] = pd.to_numeric(df_311_clean['longitude'], errors='coerce')\n",
    "    \n",
    "    # Filtrage des coordonnées valides pour Boston\n",
    "    # Boston bounds: lat [42.1, 42.7], lon [-71.3, -70.8]\n",
    "    before_geo = len(df_311_clean)\n",
    "    valid_coords = (\n",
    "        df_311_clean['latitude'].between(42.1, 42.7) & \n",
    "        df_311_clean['longitude'].between(-71.3, -70.8) &\n",
    "        df_311_clean['latitude'].notna() &\n",
    "        df_311_clean['longitude'].notna()\n",
    "    )\n",
    "    df_311_clean = df_311_clean[valid_coords]\n",
    "    after_geo = len(df_311_clean)\n",
    "    print(f\"   - Lignes supprimées (coordonnées invalides): {before_geo - after_geo:,}\")\n",
    "    \n",
    "    # ===== Nettoyage des champs textuels =====\n",
    "    print(\"\\n📝 Nettoyage des champs textuels...\")\n",
    "    \n",
    "    # Standardisation des statuts\n",
    "    status_mapping = {\n",
    "        'Closed': 'CLOSED',\n",
    "        'Open': 'OPEN',\n",
    "        'closed': 'CLOSED',\n",
    "        'open': 'OPEN'\n",
    "    }\n",
    "    if 'case_status' in df_311_clean.columns:\n",
    "        df_311_clean['case_status'] = df_311_clean['case_status'].map(status_mapping).fillna(df_311_clean['case_status'])\n",
    "    \n",
    "    # Nettoyage des noms de rues\n",
    "    if 'location_street_name' in df_311_clean.columns:\n",
    "        df_311_clean['location_street_name'] = df_311_clean['location_street_name'].str.upper().str.strip()\n",
    "    \n",
    "    # ===== Classification des priorités =====\n",
    "    print(\"\\n🎯 Classification des priorités...\")\n",
    "    \n",
    "    # Définition des catégories haute priorité\n",
    "    high_priority_keywords = [\n",
    "        'EMERGENCY', 'URGENT', 'SAFETY', 'HAZARD', 'DANGEROUS',\n",
    "        'POTHOLE', 'TRAFFIC', 'SIGNAL', 'SNOW', 'ICE'\n",
    "    ]\n",
    "    \n",
    "    def classify_priority(title, case_type):\n",
    "        \"\"\"Classifie la priorité basée sur le titre et le type\"\"\"\n",
    "        if pd.isna(title):\n",
    "            return 'LOW'\n",
    "        \n",
    "        title_upper = str(title).upper()\n",
    "        for keyword in high_priority_keywords:\n",
    "            if keyword in title_upper:\n",
    "                return 'HIGH'\n",
    "        \n",
    "        # Cas spéciaux par type\n",
    "        if 'POTHOLE' in title_upper or 'TRAFFIC' in title_upper:\n",
    "            return 'HIGH'\n",
    "        elif 'GRAFFITI' in title_upper or 'LITTER' in title_upper:\n",
    "            return 'LOW'\n",
    "        else:\n",
    "            return 'MEDIUM'\n",
    "    \n",
    "    if 'case_title' in df_311_clean.columns:\n",
    "        df_311_clean['priority'] = df_311_clean.apply(\n",
    "            lambda x: classify_priority(x.get('case_title'), x.get('type')), axis=1\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ Dataset 311 nettoyé: {len(df_311_clean):,} lignes\")\n",
    "    \n",
    "    # Statistiques de nettoyage\n",
    "    if 'priority' in df_311_clean.columns:\n",
    "        priority_counts = df_311_clean['priority'].value_counts()\n",
    "        print(\"   Répartition des priorités:\")\n",
    "        for priority, count in priority_counts.items():\n",
    "            print(f\"     - {priority}: {count:,} ({count/len(df_311_clean)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 3. NETTOYAGE DU DATASET CRIME\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚨 NETTOYAGE DU DATASET CRIME\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_crime_raw is not None:\n",
    "    df_crime_clean = df_crime_raw.copy()\n",
    "    print(f\"📋 Dataset initial: {len(df_crime_clean):,} lignes\")\n",
    "    \n",
    "    # ===== Nettoyage des dates =====\n",
    "    print(\"\\n🕐 Nettoyage des dates...\")\n",
    "    \n",
    "    # Conversion de la date d'incident\n",
    "    if 'OCCURRED_ON_DATE' in df_crime_clean.columns:\n",
    "        df_crime_clean['OCCURRED_ON_DATE'] = pd.to_datetime(df_crime_clean['OCCURRED_ON_DATE'], errors='coerce')\n",
    "        \n",
    "        # Suppression des lignes avec dates invalides\n",
    "        before_count = len(df_crime_clean)\n",
    "        df_crime_clean = df_crime_clean.dropna(subset=['OCCURRED_ON_DATE'])\n",
    "        after_count = len(df_crime_clean)\n",
    "        print(f\"   - Lignes supprimées (dates invalides): {before_count - after_count:,}\")\n",
    "        \n",
    "        # Features temporelles dérivées\n",
    "        df_crime_clean['crime_year'] = df_crime_clean['OCCURRED_ON_DATE'].dt.year\n",
    "        df_crime_clean['crime_month'] = df_crime_clean['OCCURRED_ON_DATE'].dt.month\n",
    "        df_crime_clean['crime_day'] = df_crime_clean['OCCURRED_ON_DATE'].dt.day\n",
    "        df_crime_clean['crime_weekday'] = df_crime_clean['OCCURRED_ON_DATE'].dt.dayofweek\n",
    "        df_crime_clean['crime_hour'] = df_crime_clean['OCCURRED_ON_DATE'].dt.hour\n",
    "    \n",
    "    # ===== Nettoyage des coordonnées =====\n",
    "    print(\"\\n🗺️ Nettoyage des coordonnées...\")\n",
    "    \n",
    "    # Conversion et validation\n",
    "    for col in ['Lat', 'Long']:\n",
    "        if col in df_crime_clean.columns:\n",
    "            df_crime_clean[col] = pd.to_numeric(df_crime_clean[col], errors='coerce')\n",
    "    \n",
    "    # Filtrage géographique\n",
    "    before_geo = len(df_crime_clean)\n",
    "    if 'Lat' in df_crime_clean.columns and 'Long' in df_crime_clean.columns:\n",
    "        valid_coords = (\n",
    "            df_crime_clean['Lat'].between(42.1, 42.7) & \n",
    "            df_crime_clean['Long'].between(-71.3, -70.8) &\n",
    "            df_crime_clean['Lat'].notna() &\n",
    "            df_crime_clean['Long'].notna()\n",
    "        )\n",
    "        df_crime_clean = df_crime_clean[valid_coords]\n",
    "    after_geo = len(df_crime_clean)\n",
    "    print(f\"   - Lignes supprimées (coordonnées invalides): {before_geo - after_geo:,}\")\n",
    "    \n",
    "    # ===== Classification de la gravité =====\n",
    "    print(\"\\n⚖️ Classification de la gravité...\")\n",
    "    \n",
    "    # Définition des crimes par niveau de gravité\n",
    "    high_severity_crimes = [\n",
    "        'MURDER', 'HOMICIDE', 'ASSAULT - AGGRAVATED', 'ROBBERY', \n",
    "        'RAPE', 'WEAPON', 'SHOOTING'\n",
    "    ]\n",
    "    \n",
    "    medium_severity_crimes = [\n",
    "        'BURGLARY', 'LARCENY', 'MOTOR VEHICLE ACCIDENT', 'ASSAULT - SIMPLE',\n",
    "        'VANDALISM', 'THREATS'\n",
    "    ]\n",
    "    \n",
    "    def classify_crime_severity(offense_group, description):\n",
    "        \"\"\"Classifie la gravité d'un crime\"\"\"\n",
    "        if pd.isna(offense_group):\n",
    "            return 'LOW'\n",
    "        \n",
    "        offense_upper = str(offense_group).upper()\n",
    "        \n",
    "        # Vérification haute gravité\n",
    "        for crime in high_severity_crimes:\n",
    "            if crime in offense_upper:\n",
    "                return 'HIGH'\n",
    "        \n",
    "        # Vérification gravité moyenne\n",
    "        for crime in medium_severity_crimes:\n",
    "            if crime in offense_upper:\n",
    "                return 'MEDIUM'\n",
    "        \n",
    "        # Cas spéciaux\n",
    "        if 'INVESTIGATE' in offense_upper or 'SERVICE' in offense_upper:\n",
    "            return 'LOW'\n",
    "        \n",
    "        return 'MEDIUM'\n",
    "    \n",
    "    if 'OFFENSE_CODE_GROUP' in df_crime_clean.columns:\n",
    "        df_crime_clean['severity'] = df_crime_clean.apply(\n",
    "            lambda x: classify_crime_severity(\n",
    "                x.get('OFFENSE_CODE_GROUP'), \n",
    "                x.get('OFFENSE_DESCRIPTION')\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Gestion des shootings\n",
    "        if 'SHOOTING' in df_crime_clean.columns:\n",
    "            df_crime_clean.loc[df_crime_clean['SHOOTING'] == 1, 'severity'] = 'HIGH'\n",
    "    \n",
    "    print(f\"✅ Dataset Crime nettoyé: {len(df_crime_clean):,} lignes\")\n",
    "    \n",
    "    # Statistiques de gravité\n",
    "    if 'severity' in df_crime_clean.columns:\n",
    "        severity_counts = df_crime_clean['severity'].value_counts()\n",
    "        print(\"   Répartition par gravité:\")\n",
    "        for severity, count in severity_counts.items():\n",
    "            print(f\"     - {severity}: {count:,} ({count/len(df_crime_clean)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 4. NETTOYAGE DU DATASET VISION ZERO (CRASHES)\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚗 NETTOYAGE DU DATASET VISION ZERO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_crashes_raw is not None:\n",
    "    df_crashes_clean = df_crashes_raw.copy()\n",
    "    print(f\"📋 Dataset initial: {len(df_crashes_clean):,} lignes\")\n",
    "    \n",
    "    # ===== Nettoyage des dates =====\n",
    "    print(\"\\n🕐 Nettoyage des dates...\")\n",
    "    \n",
    "    if 'dispatch_ts' in df_crashes_clean.columns:\n",
    "        df_crashes_clean['dispatch_ts'] = pd.to_datetime(df_crashes_clean['dispatch_ts'], errors='coerce')\n",
    "        \n",
    "        # Suppression des lignes avec dates invalides\n",
    "        before_count = len(df_crashes_clean)\n",
    "        df_crashes_clean = df_crashes_clean.dropna(subset=['dispatch_ts'])\n",
    "        after_count = len(df_crashes_clean)\n",
    "        print(f\"   - Lignes supprimées (dates invalides): {before_count - after_count:,}\")\n",
    "        \n",
    "        # Features temporelles\n",
    "        df_crashes_clean['crash_year'] = df_crashes_clean['dispatch_ts'].dt.year\n",
    "        df_crashes_clean['crash_month'] = df_crashes_clean['dispatch_ts'].dt.month\n",
    "        df_crashes_clean['crash_day'] = df_crashes_clean['dispatch_ts'].dt.day\n",
    "        df_crashes_clean['crash_weekday'] = df_crashes_clean['dispatch_ts'].dt.dayofweek\n",
    "        df_crashes_clean['crash_hour'] = df_crashes_clean['dispatch_ts'].dt.hour\n",
    "    \n",
    "    # ===== Nettoyage des coordonnées =====\n",
    "    print(\"\\n🗺️ Nettoyage des coordonnées...\")\n",
    "    \n",
    "    # Conversion des coordonnées\n",
    "    for col in ['lat', 'long']:\n",
    "        if col in df_crashes_clean.columns:\n",
    "            df_crashes_clean[col] = pd.to_numeric(df_crashes_clean[col], errors='coerce')\n",
    "    \n",
    "    # Filtrage géographique\n",
    "    before_geo = len(df_crashes_clean)\n",
    "    if 'lat' in df_crashes_clean.columns and 'long' in df_crashes_clean.columns:\n",
    "        valid_coords = (\n",
    "            df_crashes_clean['lat'].between(42.1, 42.7) & \n",
    "            df_crashes_clean['long'].between(-71.3, -70.8) &\n",
    "            df_crashes_clean['lat'].notna() &\n",
    "            df_crashes_clean['long'].notna()\n",
    "        )\n",
    "        df_crashes_clean = df_crashes_clean[valid_coords]\n",
    "    after_geo = len(df_crashes_clean)\n",
    "    print(f\"   - Lignes supprimées (coordonnées invalides): {before_geo - after_geo:,}\")\n",
    "    \n",
    "    # ===== Standardisation des modes de transport =====\n",
    "    print(\"\\n🚦 Standardisation des modes de transport...\")\n",
    "    \n",
    "    # Mapping des modes de transport\n",
    "    mode_mapping = {\n",
    "        'mv': 'MOTOR_VEHICLE',\n",
    "        'bike': 'BICYCLE',\n",
    "        'ped': 'PEDESTRIAN'\n",
    "    }\n",
    "    \n",
    "    if 'mode_type' in df_crashes_clean.columns:\n",
    "        df_crashes_clean['mode_type_clean'] = df_crashes_clean['mode_type'].map(mode_mapping)\n",
    "        df_crashes_clean['mode_type_clean'] = df_crashes_clean['mode_type_clean'].fillna('OTHER')\n",
    "    \n",
    "    # ===== Classification de la gravité des accidents =====\n",
    "    print(\"\\n⚖️ Classification de la gravité...\")\n",
    "    \n",
    "    def classify_crash_severity(mode_type, location_type):\n",
    "        \"\"\"Classifie la gravité d'un accident\"\"\"\n",
    "        # Les accidents impliquant piétons et vélos sont plus graves\n",
    "        if mode_type in ['PEDESTRIAN', 'BICYCLE']:\n",
    "            if location_type == 'Intersection':\n",
    "                return 'HIGH'\n",
    "            else:\n",
    "                return 'MEDIUM'\n",
    "        elif mode_type == 'MOTOR_VEHICLE':\n",
    "            if location_type == 'Intersection':\n",
    "                return 'MEDIUM'\n",
    "            else:\n",
    "                return 'LOW'\n",
    "        else:\n",
    "            return 'MEDIUM'\n",
    "    \n",
    "    if 'mode_type_clean' in df_crashes_clean.columns:\n",
    "        df_crashes_clean['crash_severity'] = df_crashes_clean.apply(\n",
    "            lambda x: classify_crash_severity(\n",
    "                x.get('mode_type_clean'), \n",
    "                x.get('location_type')\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ Dataset Crashes nettoyé: {len(df_crashes_clean):,} lignes\")\n",
    "    \n",
    "    # Statistiques des modes de transport\n",
    "    if 'mode_type_clean' in df_crashes_clean.columns:\n",
    "        mode_counts = df_crashes_clean['mode_type_clean'].value_counts()\n",
    "        print(\"   Répartition par mode de transport:\")\n",
    "        for mode, count in mode_counts.items():\n",
    "            print(f\"     - {mode}: {count:,} ({count/len(df_crashes_clean)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 5. HARMONISATION DES DATASETS\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔗 HARMONISATION DES DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== Standardisation des colonnes temporelles =====\n",
    "print(\"\\n⏰ Standardisation des colonnes temporelles...\")\n",
    "\n",
    "# Créer un format uniforme pour tous les datasets\n",
    "def standardize_temporal_columns(df, date_col, prefix):\n",
    "    \"\"\"Standardise les colonnes temporelles avec un préfixe commun\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Colonnes standardisées\n",
    "    result_df[f'{prefix}_datetime'] = df[date_col]\n",
    "    result_df[f'{prefix}_year'] = df[date_col].dt.year\n",
    "    result_df[f'{prefix}_month'] = df[date_col].dt.month\n",
    "    result_df[f'{prefix}_day'] = df[date_col].dt.day\n",
    "    result_df[f'{prefix}_weekday'] = df[date_col].dt.dayofweek\n",
    "    result_df[f'{prefix}_hour'] = df[date_col].dt.hour\n",
    "    result_df[f'{prefix}_date'] = df[date_col].dt.date\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Application de la standardisation\n",
    "standardized_datasets = {}\n",
    "\n",
    "if 'df_311_clean' in locals():\n",
    "    standardized_datasets['311'] = standardize_temporal_columns(df_311_clean, 'open_dt', 'incident')\n",
    "    # Ajout de colonnes identificatrices\n",
    "    standardized_datasets['311']['data_source'] = '311_REPORTS'\n",
    "    standardized_datasets['311']['incident_type'] = '311'\n",
    "\n",
    "if 'df_crime_clean' in locals():\n",
    "    standardized_datasets['crime'] = standardize_temporal_columns(df_crime_clean, 'OCCURRED_ON_DATE', 'incident')\n",
    "    standardized_datasets['crime']['data_source'] = 'CRIME_INCIDENTS'\n",
    "    standardized_datasets['crime']['incident_type'] = 'CRIME'\n",
    "\n",
    "if 'df_crashes_clean' in locals():\n",
    "    standardized_datasets['crashes'] = standardize_temporal_columns(df_crashes_clean, 'dispatch_ts', 'incident')\n",
    "    standardized_datasets['crashes']['data_source'] = 'TRAFFIC_CRASHES'\n",
    "    standardized_datasets['crashes']['incident_type'] = 'CRASH'\n",
    "\n",
    "# ===== Standardisation des coordonnées =====\n",
    "print(\"\\n🗺️ Standardisation des coordonnées...\")\n",
    "\n",
    "def standardize_coordinates(df, lat_col, lon_col):\n",
    "    \"\"\"Standardise les colonnes de coordonnées\"\"\"\n",
    "    result_df = df.copy()\n",
    "    result_df['latitude'] = df[lat_col]\n",
    "    result_df['longitude'] = df[lon_col]\n",
    "    return result_df\n",
    "\n",
    "# Application aux datasets\n",
    "for name, df in standardized_datasets.items():\n",
    "    if name == '311':\n",
    "        # Déjà standardisé\n",
    "        pass\n",
    "    elif name == 'crime':\n",
    "        standardized_datasets[name] = standardize_coordinates(df, 'Lat', 'Long')\n",
    "    elif name == 'crashes':\n",
    "        standardized_datasets[name] = standardize_coordinates(df, 'lat', 'long')\n",
    "\n",
    "print(\"✅ Coordonnées standardisées pour tous les datasets\")\n",
    "\n",
    "# ===== Création d'un dataset unifié minimal =====\n",
    "print(\"\\n🔄 Création d'un dataset unifié...\")\n",
    "\n",
    "unified_columns = [\n",
    "    'data_source', 'incident_type', 'incident_datetime', \n",
    "    'latitude', 'longitude', 'incident_year', 'incident_month', \n",
    "    'incident_day', 'incident_weekday', 'incident_hour'\n",
    "]\n",
    "\n",
    "unified_datasets = []\n",
    "\n",
    "for name, df in standardized_datasets.items():\n",
    "    # Sélection des colonnes communes\n",
    "    available_cols = [col for col in unified_columns if col in df.columns]\n",
    "    df_subset = df[available_cols].copy()\n",
    "    \n",
    "    # Ajout de colonnes spécifiques selon le type\n",
    "    if name == '311':\n",
    "        if 'priority' in df.columns:\n",
    "            df_subset['severity'] = df['priority']\n",
    "        else:\n",
    "            df_subset['severity'] = 'MEDIUM'\n",
    "    elif name == 'crime':\n",
    "        if 'severity' in df.columns:\n",
    "            df_subset['severity'] = df['severity']\n",
    "        else:\n",
    "            df_subset['severity'] = 'HIGH'\n",
    "    elif name == 'crashes':\n",
    "        if 'crash_severity' in df.columns:\n",
    "            df_subset['severity'] = df['crash_severity']\n",
    "        else:\n",
    "            df_subset['severity'] = 'MEDIUM'\n",
    "    \n",
    "    unified_datasets.append(df_subset)\n",
    "\n",
    "# Combinaison des datasets\n",
    "if unified_datasets:\n",
    "    df_unified = pd.concat(unified_datasets, ignore_index=True)\n",
    "    print(f\"✅ Dataset unifié créé: {len(df_unified):,} incidents\")\n",
    "    \n",
    "    # Statistiques du dataset unifié\n",
    "    print(\"\\n📊 Statistiques du dataset unifié:\")\n",
    "    source_counts = df_unified['data_source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  - {source}: {count:,} ({count/len(df_unified)*100:.1f}%)\")\n",
    "    \n",
    "    severity_counts = df_unified['severity'].value_counts()\n",
    "    print(\"\\n  Répartition par gravité globale:\")\n",
    "    for severity, count in severity_counts.items():\n",
    "        print(f\"    - {severity}: {count:,} ({count/len(df_unified)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 6. VALIDATION DE LA QUALITÉ DES DONNÉES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ VALIDATION DE LA QUALITÉ DES DONNÉES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def validate_dataset_quality(df, dataset_name):\n",
    "    \"\"\"Valide la qualité d'un dataset nettoyé\"\"\"\n",
    "    print(f\"\\n🔍 Validation pour {dataset_name}:\")\n",
    "    \n",
    "    # Comptage des valeurs manquantes\n",
    "    missing_counts = df.isnull().sum()\n",
    "    critical_columns = ['latitude', 'longitude', 'incident_datetime']\n",
    "    \n",
    "    for col in critical_columns:\n",
    "        if col in df.columns:\n",
    "            missing_count = missing_counts[col]\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            if missing_pct > 5:\n",
    "                print(f\"  ⚠️ {col}: {missing_count:,} manquantes ({missing_pct:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  ✅ {col}: {missing_count:,} manquantes ({missing_pct:.1f}%)\")\n",
    "    \n",
    "    # Validation temporelle\n",
    "    if 'incident_datetime' in df.columns:\n",
    "        # Calcul de la période couverte\n",
    "        try:\n",
    "            date_range = df['incident_datetime'].max() - df['incident_datetime'].min()\n",
    "            print(f\"  📅 Période couverte: {date_range.days} jours\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Erreur calcul période: {str(e)}\")\n",
    "        \n",
    "        # Détection d'anomalies temporelles - version robuste\n",
    "        try:\n",
    "            # Convertir toutes les dates en naive datetime pour comparaison\n",
    "            current_time = pd.Timestamp.now()\n",
    "            \n",
    "            # Si les colonnes ont une timezone, on la supprime\n",
    "            if hasattr(df['incident_datetime'].dtype, 'tz') and df['incident_datetime'].dtype.tz is not None:\n",
    "                incident_dates = df['incident_datetime'].dt.tz_localize(None)\n",
    "            else:\n",
    "                incident_dates = df['incident_datetime']\n",
    "            \n",
    "            # S'assurer que current_time est aussi naive\n",
    "            if hasattr(current_time, 'tz') and current_time.tz is not None:\n",
    "                current_time = current_time.tz_localize(None)\n",
    "            \n",
    "            # Comparaison sécurisée\n",
    "            future_mask = incident_dates > current_time\n",
    "            future_count = future_mask.sum()\n",
    "            \n",
    "            if future_count > 0:\n",
    "                print(f\"  ⚠️ Dates futures détectées: {future_count:,}\")\n",
    "            else:\n",
    "                print(f\"  ✅ Aucune date future détectée\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Impossible de vérifier les dates futures: {str(e)}\")\n",
    "    \n",
    "    # Validation géospatiale\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        try:\n",
    "            coord_bounds = {\n",
    "                'lat_min': df['latitude'].min(),\n",
    "                'lat_max': df['latitude'].max(),\n",
    "                'lon_min': df['longitude'].min(),\n",
    "                'lon_max': df['longitude'].max()\n",
    "            }\n",
    "            \n",
    "            # Vérification des limites de Boston\n",
    "            boston_bounds = {\n",
    "                'lat_min': 42.1, 'lat_max': 42.7,\n",
    "                'lon_min': -71.3, 'lon_max': -70.8\n",
    "            }\n",
    "            \n",
    "            outside_bounds = (\n",
    "                (df['latitude'] < boston_bounds['lat_min']) |\n",
    "                (df['latitude'] > boston_bounds['lat_max']) |\n",
    "                (df['longitude'] < boston_bounds['lon_min']) |\n",
    "                (df['longitude'] > boston_bounds['lon_max'])\n",
    "            ).sum()\n",
    "            \n",
    "            if outside_bounds > 0:\n",
    "                print(f\"  ⚠️ Points hors limites Boston: {outside_bounds:,}\")\n",
    "            else:\n",
    "                print(f\"  ✅ Toutes les coordonnées dans les limites de Boston\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Erreur validation géospatiale: {str(e)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Validation des datasets nettoyés\n",
    "if standardized_datasets:\n",
    "    for name, df in standardized_datasets.items():\n",
    "        validate_dataset_quality(df, name.upper())\n",
    "\n",
    "if 'df_unified' in locals():\n",
    "    validate_dataset_quality(df_unified, \"DATASET UNIFIÉ\")\n",
    "\n",
    "# =====================================================================\n",
    "# 7. SAUVEGARDE DES DONNÉES NETTOYÉES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"💾 SAUVEGARDE DES DONNÉES NETTOYÉES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "\n",
    "# Création du dossier de sortie\n",
    "output_dir = 'data/processed'\n",
    "os.makedirs(\"../\" + output_dir, exist_ok=True)\n",
    "\n",
    "# Fonction de sauvegarde flexible\n",
    "def save_dataframe(df, filename_base, output_dir):\n",
    "    \"\"\"Sauvegarde un dataframe en essayant plusieurs formats\"\"\"\n",
    "    saved_path = None\n",
    "    \n",
    "    # Essayer Parquet d'abord (plus efficace)\n",
    "    try:\n",
    "        parquet_path = f\"../{output_dir}/{filename_base}.parquet\"\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        saved_path = parquet_path\n",
    "        print(f\"✅ Sauvegardé: {parquet_path} ({len(df):,} lignes) [Parquet]\")\n",
    "    except ImportError:\n",
    "        # Si Parquet n'est pas disponible, utiliser CSV\n",
    "        try:\n",
    "            csv_path = f\"../{output_dir}/{filename_base}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            saved_path = csv_path\n",
    "            print(f\"✅ Sauvegardé: {csv_path} ({len(df):,} lignes) [CSV - Parquet non disponible]\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur sauvegarde {filename_base}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur sauvegarde Parquet {filename_base}: {e}\")\n",
    "        # Fallback vers CSV\n",
    "        try:\n",
    "            csv_path = f\"../{output_dir}/{filename_base}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            saved_path = csv_path\n",
    "            print(f\"✅ Sauvegardé: {csv_path} ({len(df):,} lignes) [CSV - Fallback]\")\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Erreur sauvegarde CSV {filename_base}: {e2}\")\n",
    "    \n",
    "    return saved_path\n",
    "\n",
    "# Sauvegarde des datasets individuels\n",
    "saved_files = []\n",
    "\n",
    "for name, df in standardized_datasets.items():\n",
    "    saved_path = save_dataframe(df, f\"cleaned_{name}\", output_dir)\n",
    "    if saved_path:\n",
    "        saved_files.append(saved_path)\n",
    "\n",
    "# Sauvegarde du dataset unifié\n",
    "if 'df_unified' in locals():\n",
    "    saved_path = save_dataframe(df_unified, \"unified_incidents\", output_dir)\n",
    "    if saved_path:\n",
    "        saved_files.append(saved_path)\n",
    "\n",
    "# Message d'installation de pyarrow si nécessaire\n",
    "if any('.csv' in path for path in saved_files):\n",
    "    print(\"\\n💡 CONSEIL:\")\n",
    "    print(\"   Pour des performances optimales, installez pyarrow:\")\n",
    "    print(\"   pip install pyarrow\")\n",
    "    print(\"   Cela permettra l'utilisation du format Parquet (plus rapide et compact)\")\n",
    "\n",
    "# Sauvegarde des métadonnées\n",
    "def get_temporal_coverage(df):\n",
    "    \"\"\"Calcule la couverture temporelle en gérant les timezones\"\"\"\n",
    "    try:\n",
    "        if 'incident_datetime' not in df.columns:\n",
    "            return \"N/A\"\n",
    "        \n",
    "        # Normalisation des timezones pour le calcul\n",
    "        datetime_series = df['incident_datetime'].copy()\n",
    "        \n",
    "        # Conversion robuste des timezones\n",
    "        if pd.api.types.is_datetime64tz_dtype(datetime_series):\n",
    "            # Si c'est un datetime avec timezone, convertir en naive\n",
    "            datetime_series = datetime_series.dt.tz_convert(None)\n",
    "        elif pd.api.types.is_datetime64_dtype(datetime_series):\n",
    "            # Si c'est déjà un datetime naive, pas de changement\n",
    "            pass\n",
    "        else:\n",
    "            # Conversion vers datetime si ce n'est pas déjà le cas\n",
    "            datetime_series = pd.to_datetime(datetime_series)\n",
    "        \n",
    "        # Assurer que c'est bien naive maintenant\n",
    "        if pd.api.types.is_datetime64tz_dtype(datetime_series):\n",
    "            datetime_series = datetime_series.dt.tz_localize(None)\n",
    "        \n",
    "        # Calcul sécurisé du min et max\n",
    "        min_date = datetime_series.min()\n",
    "        max_date = datetime_series.max()\n",
    "        \n",
    "        # Conversion en string pour éviter les problèmes de sérialisation JSON\n",
    "        return f\"{str(min_date)} to {str(max_date)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "metadata = {\n",
    "    'preprocessing_date': datetime.now().isoformat(),\n",
    "    'datasets_processed': list(standardized_datasets.keys()),\n",
    "    'total_incidents': len(df_unified) if 'df_unified' in locals() else 0,\n",
    "    'data_quality': {\n",
    "        'coordinate_coverage': len(df_unified[df_unified[['latitude', 'longitude']].notna().all(axis=1)]) / len(df_unified) * 100 if 'df_unified' in locals() else 0,\n",
    "        'temporal_coverage': get_temporal_coverage(df_unified) if 'df_unified' in locals() else \"N/A\"\n",
    "    },\n",
    "    'file_formats_used': ['parquet' if any('.parquet' in path for path in saved_files) else 'csv']\n",
    "}\n",
    "\n",
    "metadata_filename = f\"../{output_dir}/preprocessing_metadata.json\"\n",
    "import json\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Métadonnées sauvegardées: {metadata_filename}\")\n",
    "\n",
    "# =====================================================================\n",
    "# 8. RÉSUMÉ DU PREPROCESSING\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 RÉSUMÉ DU PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n🎯 DATASETS TRAITÉS:\")\n",
    "if standardized_datasets:\n",
    "    for name, df in standardized_datasets.items():\n",
    "        print(f\"  • {name.upper()}: {len(df):,} incidents\")\n",
    "\n",
    "if 'df_unified' in locals():\n",
    "    print(f\"\\n📊 DATASET UNIFIÉ: {len(df_unified):,} incidents totaux\")\n",
    "    \n",
    "    # Calcul sécurisé de la période avec gestion de timezone\n",
    "    try:\n",
    "        # Normalisation des timezones pour l'affichage\n",
    "        datetime_series = df_unified['incident_datetime']\n",
    "        if hasattr(datetime_series.dtype, 'tz') and datetime_series.dtype.tz is not None:\n",
    "            datetime_series = datetime_series.dt.tz_localize(None)\n",
    "        \n",
    "        min_date = datetime_series.min()\n",
    "        max_date = datetime_series.max()\n",
    "        print(f\"  • Période: {min_date} à {max_date}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  • Période: Erreur calcul dates - {str(e)}\")\n",
    "    \n",
    "    print(f\"  • Couverture géographique: {len(df_unified[df_unified[['latitude', 'longitude']].notna().all(axis=1)])} incidents avec coordonnées\")\n",
    "\n",
    "print(f\"\\n💾 FICHIERS SAUVEGARDÉS:\")\n",
    "for filename in saved_files:\n",
    "    print(f\"  • {filename}\")\n",
    "\n",
    "print(f\"\\n✅ QUALITÉ DES DONNÉES:\")\n",
    "print(f\"  • Coordonnées valides: {metadata['data_quality']['coordinate_coverage']:.1f}%\")\n",
    "print(f\"  • Période temporelle: {metadata['data_quality']['temporal_coverage']}\")\n",
    "\n",
    "print(\"\\n🚀 PROCHAINE ÉTAPE:\")\n",
    "print(\"  Les données sont prêtes pour le feature engineering!\")\n",
    "print(\"  Prochain notebook: 03_feature_engineering.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✨ PREPROCESSING TERMINÉ AVEC SUCCÈS\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
