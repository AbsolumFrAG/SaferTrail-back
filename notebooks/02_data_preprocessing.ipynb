{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f808c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß D√©but du preprocessing des donn√©es\n",
      "============================================================\n",
      "\n",
      "üìÅ Chargement des datasets...\n",
      "\n",
      "üìä √âtat du chargement:\n",
      "  311 Reports: ‚úÖ Charg√©\n",
      "  Crime Incidents: ‚úÖ Charg√©\n",
      "  Vision Zero Crashes: ‚úÖ Charg√©\n",
      "\n",
      "============================================================\n",
      "üßπ NETTOYAGE DU DATASET 311\n",
      "============================================================\n",
      "üìã Dataset initial: 709 lignes\n",
      "\n",
      "üïê Nettoyage des dates...\n",
      "   - Lignes supprim√©es (dates invalides): 0\n",
      "\n",
      "üó∫Ô∏è Nettoyage des coordonn√©es...\n",
      "   - Lignes supprim√©es (coordonn√©es invalides): 2\n",
      "\n",
      "üìù Nettoyage des champs textuels...\n",
      "\n",
      "üéØ Classification des priorit√©s...\n",
      "‚úÖ Dataset 311 nettoy√©: 707 lignes\n",
      "   R√©partition des priorit√©s:\n",
      "     - MEDIUM: 707 (100.0%)\n",
      "\n",
      "============================================================\n",
      "üö® NETTOYAGE DU DATASET CRIME\n",
      "============================================================\n",
      "üìã Dataset initial: 185,534 lignes\n",
      "\n",
      "üïê Nettoyage des dates...\n",
      "   - Lignes supprim√©es (dates invalides): 0\n",
      "\n",
      "üó∫Ô∏è Nettoyage des coordonn√©es...\n",
      "   - Lignes supprim√©es (coordonn√©es invalides): 11,468\n",
      "\n",
      "‚öñÔ∏è Classification de la gravit√©...\n",
      "‚úÖ Dataset Crime nettoy√©: 174,066 lignes\n",
      "   R√©partition par gravit√©:\n",
      "     - LOW: 172,803 (99.3%)\n",
      "     - HIGH: 1,263 (0.7%)\n",
      "\n",
      "============================================================\n",
      "üöó NETTOYAGE DU DATASET VISION ZERO\n",
      "============================================================\n",
      "üìã Dataset initial: 37,896 lignes\n",
      "\n",
      "üïê Nettoyage des dates...\n",
      "   - Lignes supprim√©es (dates invalides): 0\n",
      "\n",
      "üó∫Ô∏è Nettoyage des coordonn√©es...\n",
      "   - Lignes supprim√©es (coordonn√©es invalides): 0\n",
      "\n",
      "üö¶ Standardisation des modes de transport...\n",
      "\n",
      "‚öñÔ∏è Classification de la gravit√©...\n",
      "‚úÖ Dataset Crashes nettoy√©: 37,896 lignes\n",
      "   R√©partition par mode de transport:\n",
      "     - MOTOR_VEHICLE: 28,173 (74.3%)\n",
      "     - PEDESTRIAN: 6,083 (16.1%)\n",
      "     - BICYCLE: 3,640 (9.6%)\n",
      "\n",
      "============================================================\n",
      "üîó HARMONISATION DES DATASETS\n",
      "============================================================\n",
      "\n",
      "‚è∞ Standardisation des colonnes temporelles...\n",
      "\n",
      "üó∫Ô∏è Standardisation des coordonn√©es...\n",
      "‚úÖ Coordonn√©es standardis√©es pour tous les datasets\n",
      "\n",
      "üîÑ Cr√©ation d'un dataset unifi√©...\n",
      "‚úÖ Dataset unifi√© cr√©√©: 212,669 incidents\n",
      "\n",
      "üìä Statistiques du dataset unifi√©:\n",
      "  - CRIME_INCIDENTS: 174,066 (81.8%)\n",
      "  - TRAFFIC_CRASHES: 37,896 (17.8%)\n",
      "  - 311_REPORTS: 707 (0.3%)\n",
      "\n",
      "  R√©partition par gravit√© globale:\n",
      "    - LOW: 187,861 (88.3%)\n",
      "    - MEDIUM: 18,788 (8.8%)\n",
      "    - HIGH: 6,020 (2.8%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ VALIDATION DE LA QUALIT√â DES DONN√âES\n",
      "============================================================\n",
      "\n",
      "üîç Validation pour 311:\n",
      "  ‚úÖ latitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ longitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ incident_datetime: 0 manquantes (0.0%)\n",
      "  üìÖ P√©riode couverte: 137 jours\n",
      "  ‚úÖ Aucune date future d√©tect√©e\n",
      "  ‚úÖ Toutes les coordonn√©es dans les limites de Boston\n",
      "\n",
      "üîç Validation pour CRIME:\n",
      "  ‚úÖ latitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ longitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ incident_datetime: 0 manquantes (0.0%)\n",
      "  üìÖ P√©riode couverte: 868 jours\n",
      "  ‚úÖ Aucune date future d√©tect√©e\n",
      "  ‚úÖ Toutes les coordonn√©es dans les limites de Boston\n",
      "\n",
      "üîç Validation pour CRASHES:\n",
      "  ‚úÖ latitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ longitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ incident_datetime: 0 manquantes (0.0%)\n",
      "  üìÖ P√©riode couverte: 3652 jours\n",
      "  ‚úÖ Aucune date future d√©tect√©e\n",
      "  ‚úÖ Toutes les coordonn√©es dans les limites de Boston\n",
      "\n",
      "üîç Validation pour DATASET UNIFI√â:\n",
      "  ‚úÖ latitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ longitude: 0 manquantes (0.0%)\n",
      "  ‚úÖ incident_datetime: 0 manquantes (0.0%)\n",
      "  ‚ö†Ô∏è Erreur calcul p√©riode: Cannot compare tz-naive and tz-aware timestamps\n",
      "  ‚ö†Ô∏è Impossible de v√©rifier les dates futures: Cannot compare tz-naive and tz-aware timestamps\n",
      "  ‚úÖ Toutes les coordonn√©es dans les limites de Boston\n",
      "\n",
      "============================================================\n",
      "üíæ SAUVEGARDE DES DONN√âES NETTOY√âES\n",
      "============================================================\n",
      "‚úÖ Sauvegard√©: ../data/processed/cleaned_311.parquet (707 lignes) [Parquet]\n",
      "‚ùå Erreur sauvegarde Parquet cleaned_crime: (\"Could not convert '232044120' with type str: tried to convert to int64\", 'Conversion failed for column INCIDENT_NUMBER with type object')\n",
      "‚úÖ Sauvegard√©: ../data/processed/cleaned_crime.csv (174,066 lignes) [CSV - Fallback]\n",
      "‚úÖ Sauvegard√©: ../data/processed/cleaned_crashes.parquet (37,896 lignes) [Parquet]\n",
      "‚úÖ Sauvegard√©: ../data/processed/unified_incidents.parquet (212,669 lignes) [Parquet]\n",
      "\n",
      "üí° CONSEIL:\n",
      "   Pour des performances optimales, installez pyarrow:\n",
      "   pip install pyarrow\n",
      "   Cela permettra l'utilisation du format Parquet (plus rapide et compact)\n",
      "‚úÖ M√©tadonn√©es sauvegard√©es: ../data/processed/preprocessing_metadata.json\n",
      "\n",
      "============================================================\n",
      "üìã R√âSUM√â DU PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "üéØ DATASETS TRAIT√âS:\n",
      "  ‚Ä¢ 311: 707 incidents\n",
      "  ‚Ä¢ CRIME: 174,066 incidents\n",
      "  ‚Ä¢ CRASHES: 37,896 incidents\n",
      "\n",
      "üìä DATASET UNIFI√â: 212,669 incidents totaux\n",
      "  ‚Ä¢ P√©riode: Erreur calcul dates - Cannot compare tz-naive and tz-aware timestamps\n",
      "  ‚Ä¢ Couverture g√©ographique: 212669 incidents avec coordonn√©es\n",
      "\n",
      "üíæ FICHIERS SAUVEGARD√âS:\n",
      "  ‚Ä¢ ../data/processed/cleaned_311.parquet\n",
      "  ‚Ä¢ ../data/processed/cleaned_crime.csv\n",
      "  ‚Ä¢ ../data/processed/cleaned_crashes.parquet\n",
      "  ‚Ä¢ ../data/processed/unified_incidents.parquet\n",
      "\n",
      "‚úÖ QUALIT√â DES DONN√âES:\n",
      "  ‚Ä¢ Coordonn√©es valides: 100.0%\n",
      "  ‚Ä¢ P√©riode temporelle: Error: Tz-aware datetime.datetime cannot be converted to datetime64 unless utc=True, at position 707\n",
      "\n",
      "üöÄ PROCHAINE √âTAPE:\n",
      "  Les donn√©es sont pr√™tes pour le feature engineering!\n",
      "  Prochain notebook: 03_feature_engineering.ipynb\n",
      "\n",
      "============================================================\n",
      "‚ú® PREPROCESSING TERMIN√â AVEC SUCC√àS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing des Donn√©es - Mod√®le de Pr√©diction de Rues Risqu√©es\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Utilitaires pour le nettoyage g√©ospatial\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import folium\n",
    "\n",
    "print(\"üîß D√©but du preprocessing des donn√©es\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# =====================================================================\n",
    "# 1. CHARGEMENT DES DONN√âES BRUTES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\nüìÅ Chargement des datasets...\")\n",
    "\n",
    "def load_csv_file(filename):\n",
    "    try:\n",
    "        return pd.read_csv(filename)\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            return pd.read_csv(f'../data/raw/{filename}')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è Fichier non trouv√©: {filename}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors du chargement de {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Chargement des datasets\n",
    "df_311_raw = load_csv_file('311.csv')\n",
    "df_crime_raw = load_csv_file('crime-incidents-report.csv')\n",
    "df_crashes_raw = load_csv_file('vision-zero-crash-records.csv')\n",
    "\n",
    "# V√©rification du chargement\n",
    "datasets_loaded = {\n",
    "    '311 Reports': df_311_raw is not None,\n",
    "    'Crime Incidents': df_crime_raw is not None,\n",
    "    'Vision Zero Crashes': df_crashes_raw is not None\n",
    "}\n",
    "\n",
    "print(\"\\nüìä √âtat du chargement:\")\n",
    "for name, loaded in datasets_loaded.items():\n",
    "    status = \"‚úÖ Charg√©\" if loaded else \"‚ùå √âchec\"\n",
    "    print(f\"  {name}: {status}\")\n",
    "\n",
    "# =====================================================================\n",
    "# 2. NETTOYAGE DU DATASET 311\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üßπ NETTOYAGE DU DATASET 311\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_311_raw is not None:\n",
    "    df_311_clean = df_311_raw.copy()\n",
    "    print(f\"üìã Dataset initial: {len(df_311_clean):,} lignes\")\n",
    "    \n",
    "    # ===== Nettoyage des dates =====\n",
    "    print(\"\\nüïê Nettoyage des dates...\")\n",
    "    \n",
    "    # Conversion des colonnes de date\n",
    "    date_columns = ['open_dt', 'sla_target_dt', 'closed_dt']\n",
    "    for col in date_columns:\n",
    "        if col in df_311_clean.columns:\n",
    "            df_311_clean[col] = pd.to_datetime(df_311_clean[col], errors='coerce')\n",
    "    \n",
    "    # Suppression des lignes avec des dates d'ouverture invalides\n",
    "    before_count = len(df_311_clean)\n",
    "    df_311_clean = df_311_clean.dropna(subset=['open_dt'])\n",
    "    after_count = len(df_311_clean)\n",
    "    print(f\"   - Lignes supprim√©es (dates invalides): {before_count - after_count:,}\")\n",
    "    \n",
    "    # Ajout de features temporelles d√©riv√©es\n",
    "    df_311_clean['open_year'] = df_311_clean['open_dt'].dt.year\n",
    "    df_311_clean['open_month'] = df_311_clean['open_dt'].dt.month\n",
    "    df_311_clean['open_day'] = df_311_clean['open_dt'].dt.day\n",
    "    df_311_clean['open_weekday'] = df_311_clean['open_dt'].dt.dayofweek\n",
    "    df_311_clean['open_hour'] = df_311_clean['open_dt'].dt.hour\n",
    "    \n",
    "    # ===== Nettoyage des coordonn√©es g√©ographiques =====\n",
    "    print(\"\\nüó∫Ô∏è Nettoyage des coordonn√©es...\")\n",
    "    \n",
    "    # Conversion en num√©rique\n",
    "    df_311_clean['latitude'] = pd.to_numeric(df_311_clean['latitude'], errors='coerce')\n",
    "    df_311_clean['longitude'] = pd.to_numeric(df_311_clean['longitude'], errors='coerce')\n",
    "    \n",
    "    # Filtrage des coordonn√©es valides pour Boston\n",
    "    # Boston bounds: lat [42.1, 42.7], lon [-71.3, -70.8]\n",
    "    before_geo = len(df_311_clean)\n",
    "    valid_coords = (\n",
    "        df_311_clean['latitude'].between(42.1, 42.7) & \n",
    "        df_311_clean['longitude'].between(-71.3, -70.8) &\n",
    "        df_311_clean['latitude'].notna() &\n",
    "        df_311_clean['longitude'].notna()\n",
    "    )\n",
    "    df_311_clean = df_311_clean[valid_coords]\n",
    "    after_geo = len(df_311_clean)\n",
    "    print(f\"   - Lignes supprim√©es (coordonn√©es invalides): {before_geo - after_geo:,}\")\n",
    "    \n",
    "    # ===== Nettoyage des champs textuels =====\n",
    "    print(\"\\nüìù Nettoyage des champs textuels...\")\n",
    "    \n",
    "    # Standardisation des statuts\n",
    "    status_mapping = {\n",
    "        'Closed': 'CLOSED',\n",
    "        'Open': 'OPEN',\n",
    "        'closed': 'CLOSED',\n",
    "        'open': 'OPEN'\n",
    "    }\n",
    "    if 'case_status' in df_311_clean.columns:\n",
    "        df_311_clean['case_status'] = df_311_clean['case_status'].map(status_mapping).fillna(df_311_clean['case_status'])\n",
    "    \n",
    "    # Nettoyage des noms de rues\n",
    "    if 'location_street_name' in df_311_clean.columns:\n",
    "        df_311_clean['location_street_name'] = df_311_clean['location_street_name'].str.upper().str.strip()\n",
    "    \n",
    "    # ===== Classification des priorit√©s =====\n",
    "    print(\"\\nüéØ Classification des priorit√©s...\")\n",
    "    \n",
    "    # D√©finition des cat√©gories haute priorit√©\n",
    "    high_priority_keywords = [\n",
    "        'EMERGENCY', 'URGENT', 'SAFETY', 'HAZARD', 'DANGEROUS',\n",
    "        'POTHOLE', 'TRAFFIC', 'SIGNAL', 'SNOW', 'ICE'\n",
    "    ]\n",
    "    \n",
    "    def classify_priority(title, case_type):\n",
    "        \"\"\"Classifie la priorit√© bas√©e sur le titre et le type\"\"\"\n",
    "        if pd.isna(title):\n",
    "            return 'LOW'\n",
    "        \n",
    "        title_upper = str(title).upper()\n",
    "        for keyword in high_priority_keywords:\n",
    "            if keyword in title_upper:\n",
    "                return 'HIGH'\n",
    "        \n",
    "        # Cas sp√©ciaux par type\n",
    "        if 'POTHOLE' in title_upper or 'TRAFFIC' in title_upper:\n",
    "            return 'HIGH'\n",
    "        elif 'GRAFFITI' in title_upper or 'LITTER' in title_upper:\n",
    "            return 'LOW'\n",
    "        else:\n",
    "            return 'MEDIUM'\n",
    "    \n",
    "    if 'case_title' in df_311_clean.columns:\n",
    "        df_311_clean['priority'] = df_311_clean.apply(\n",
    "            lambda x: classify_priority(x.get('case_title'), x.get('type')), axis=1\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset 311 nettoy√©: {len(df_311_clean):,} lignes\")\n",
    "    \n",
    "    # Statistiques de nettoyage\n",
    "    if 'priority' in df_311_clean.columns:\n",
    "        priority_counts = df_311_clean['priority'].value_counts()\n",
    "        print(\"   R√©partition des priorit√©s:\")\n",
    "        for priority, count in priority_counts.items():\n",
    "            print(f\"     - {priority}: {count:,} ({count/len(df_311_clean)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 3. NETTOYAGE DU DATASET CRIME\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üö® NETTOYAGE DU DATASET CRIME\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_crime_raw is not None:\n",
    "    df_crime_clean = df_crime_raw.copy()\n",
    "    print(f\"üìã Dataset initial: {len(df_crime_clean):,} lignes\")\n",
    "    \n",
    "    # ===== Nettoyage des dates =====\n",
    "    print(\"\\nüïê Nettoyage des dates...\")\n",
    "    \n",
    "    # Conversion de la date d'incident\n",
    "    if 'OCCURRED_ON_DATE' in df_crime_clean.columns:\n",
    "        df_crime_clean['OCCURRED_ON_DATE'] = pd.to_datetime(df_crime_clean['OCCURRED_ON_DATE'], errors='coerce')\n",
    "        \n",
    "        # Suppression des lignes avec dates invalides\n",
    "        before_count = len(df_crime_clean)\n",
    "        df_crime_clean = df_crime_clean.dropna(subset=['OCCURRED_ON_DATE'])\n",
    "        after_count = len(df_crime_clean)\n",
    "        print(f\"   - Lignes supprim√©es (dates invalides): {before_count - after_count:,}\")\n",
    "        \n",
    "        # Features temporelles d√©riv√©es\n",
    "        df_crime_clean['crime_year'] = df_crime_clean['OCCURRED_ON_DATE'].dt.year\n",
    "        df_crime_clean['crime_month'] = df_crime_clean['OCCURRED_ON_DATE'].dt.month\n",
    "        df_crime_clean['crime_day'] = df_crime_clean['OCCURRED_ON_DATE'].dt.day\n",
    "        df_crime_clean['crime_weekday'] = df_crime_clean['OCCURRED_ON_DATE'].dt.dayofweek\n",
    "        df_crime_clean['crime_hour'] = df_crime_clean['OCCURRED_ON_DATE'].dt.hour\n",
    "    \n",
    "    # ===== Nettoyage des coordonn√©es =====\n",
    "    print(\"\\nüó∫Ô∏è Nettoyage des coordonn√©es...\")\n",
    "    \n",
    "    # Conversion et validation\n",
    "    for col in ['Lat', 'Long']:\n",
    "        if col in df_crime_clean.columns:\n",
    "            df_crime_clean[col] = pd.to_numeric(df_crime_clean[col], errors='coerce')\n",
    "    \n",
    "    # Filtrage g√©ographique\n",
    "    before_geo = len(df_crime_clean)\n",
    "    if 'Lat' in df_crime_clean.columns and 'Long' in df_crime_clean.columns:\n",
    "        valid_coords = (\n",
    "            df_crime_clean['Lat'].between(42.1, 42.7) & \n",
    "            df_crime_clean['Long'].between(-71.3, -70.8) &\n",
    "            df_crime_clean['Lat'].notna() &\n",
    "            df_crime_clean['Long'].notna()\n",
    "        )\n",
    "        df_crime_clean = df_crime_clean[valid_coords]\n",
    "    after_geo = len(df_crime_clean)\n",
    "    print(f\"   - Lignes supprim√©es (coordonn√©es invalides): {before_geo - after_geo:,}\")\n",
    "    \n",
    "    # ===== Classification de la gravit√© =====\n",
    "    print(\"\\n‚öñÔ∏è Classification de la gravit√©...\")\n",
    "    \n",
    "    # D√©finition des crimes par niveau de gravit√©\n",
    "    high_severity_crimes = [\n",
    "        'MURDER', 'HOMICIDE', 'ASSAULT - AGGRAVATED', 'ROBBERY', \n",
    "        'RAPE', 'WEAPON', 'SHOOTING'\n",
    "    ]\n",
    "    \n",
    "    medium_severity_crimes = [\n",
    "        'BURGLARY', 'LARCENY', 'MOTOR VEHICLE ACCIDENT', 'ASSAULT - SIMPLE',\n",
    "        'VANDALISM', 'THREATS'\n",
    "    ]\n",
    "    \n",
    "    def classify_crime_severity(offense_group, description):\n",
    "        \"\"\"Classifie la gravit√© d'un crime\"\"\"\n",
    "        if pd.isna(offense_group):\n",
    "            return 'LOW'\n",
    "        \n",
    "        offense_upper = str(offense_group).upper()\n",
    "        \n",
    "        # V√©rification haute gravit√©\n",
    "        for crime in high_severity_crimes:\n",
    "            if crime in offense_upper:\n",
    "                return 'HIGH'\n",
    "        \n",
    "        # V√©rification gravit√© moyenne\n",
    "        for crime in medium_severity_crimes:\n",
    "            if crime in offense_upper:\n",
    "                return 'MEDIUM'\n",
    "        \n",
    "        # Cas sp√©ciaux\n",
    "        if 'INVESTIGATE' in offense_upper or 'SERVICE' in offense_upper:\n",
    "            return 'LOW'\n",
    "        \n",
    "        return 'MEDIUM'\n",
    "    \n",
    "    if 'OFFENSE_CODE_GROUP' in df_crime_clean.columns:\n",
    "        df_crime_clean['severity'] = df_crime_clean.apply(\n",
    "            lambda x: classify_crime_severity(\n",
    "                x.get('OFFENSE_CODE_GROUP'), \n",
    "                x.get('OFFENSE_DESCRIPTION')\n",
    "            ), axis=1\n",
    "        )\n",
    "        \n",
    "        # Gestion des shootings\n",
    "        if 'SHOOTING' in df_crime_clean.columns:\n",
    "            df_crime_clean.loc[df_crime_clean['SHOOTING'] == 1, 'severity'] = 'HIGH'\n",
    "    \n",
    "    print(f\"‚úÖ Dataset Crime nettoy√©: {len(df_crime_clean):,} lignes\")\n",
    "    \n",
    "    # Statistiques de gravit√©\n",
    "    if 'severity' in df_crime_clean.columns:\n",
    "        severity_counts = df_crime_clean['severity'].value_counts()\n",
    "        print(\"   R√©partition par gravit√©:\")\n",
    "        for severity, count in severity_counts.items():\n",
    "            print(f\"     - {severity}: {count:,} ({count/len(df_crime_clean)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 4. NETTOYAGE DU DATASET VISION ZERO (CRASHES)\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöó NETTOYAGE DU DATASET VISION ZERO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_crashes_raw is not None:\n",
    "    df_crashes_clean = df_crashes_raw.copy()\n",
    "    print(f\"üìã Dataset initial: {len(df_crashes_clean):,} lignes\")\n",
    "    \n",
    "    # ===== Nettoyage des dates =====\n",
    "    print(\"\\nüïê Nettoyage des dates...\")\n",
    "    \n",
    "    if 'dispatch_ts' in df_crashes_clean.columns:\n",
    "        df_crashes_clean['dispatch_ts'] = pd.to_datetime(df_crashes_clean['dispatch_ts'], errors='coerce')\n",
    "        \n",
    "        # Suppression des lignes avec dates invalides\n",
    "        before_count = len(df_crashes_clean)\n",
    "        df_crashes_clean = df_crashes_clean.dropna(subset=['dispatch_ts'])\n",
    "        after_count = len(df_crashes_clean)\n",
    "        print(f\"   - Lignes supprim√©es (dates invalides): {before_count - after_count:,}\")\n",
    "        \n",
    "        # Features temporelles\n",
    "        df_crashes_clean['crash_year'] = df_crashes_clean['dispatch_ts'].dt.year\n",
    "        df_crashes_clean['crash_month'] = df_crashes_clean['dispatch_ts'].dt.month\n",
    "        df_crashes_clean['crash_day'] = df_crashes_clean['dispatch_ts'].dt.day\n",
    "        df_crashes_clean['crash_weekday'] = df_crashes_clean['dispatch_ts'].dt.dayofweek\n",
    "        df_crashes_clean['crash_hour'] = df_crashes_clean['dispatch_ts'].dt.hour\n",
    "    \n",
    "    # ===== Nettoyage des coordonn√©es =====\n",
    "    print(\"\\nüó∫Ô∏è Nettoyage des coordonn√©es...\")\n",
    "    \n",
    "    # Conversion des coordonn√©es\n",
    "    for col in ['lat', 'long']:\n",
    "        if col in df_crashes_clean.columns:\n",
    "            df_crashes_clean[col] = pd.to_numeric(df_crashes_clean[col], errors='coerce')\n",
    "    \n",
    "    # Filtrage g√©ographique\n",
    "    before_geo = len(df_crashes_clean)\n",
    "    if 'lat' in df_crashes_clean.columns and 'long' in df_crashes_clean.columns:\n",
    "        valid_coords = (\n",
    "            df_crashes_clean['lat'].between(42.1, 42.7) & \n",
    "            df_crashes_clean['long'].between(-71.3, -70.8) &\n",
    "            df_crashes_clean['lat'].notna() &\n",
    "            df_crashes_clean['long'].notna()\n",
    "        )\n",
    "        df_crashes_clean = df_crashes_clean[valid_coords]\n",
    "    after_geo = len(df_crashes_clean)\n",
    "    print(f\"   - Lignes supprim√©es (coordonn√©es invalides): {before_geo - after_geo:,}\")\n",
    "    \n",
    "    # ===== Standardisation des modes de transport =====\n",
    "    print(\"\\nüö¶ Standardisation des modes de transport...\")\n",
    "    \n",
    "    # Mapping des modes de transport\n",
    "    mode_mapping = {\n",
    "        'mv': 'MOTOR_VEHICLE',\n",
    "        'bike': 'BICYCLE',\n",
    "        'ped': 'PEDESTRIAN'\n",
    "    }\n",
    "    \n",
    "    if 'mode_type' in df_crashes_clean.columns:\n",
    "        df_crashes_clean['mode_type_clean'] = df_crashes_clean['mode_type'].map(mode_mapping)\n",
    "        df_crashes_clean['mode_type_clean'] = df_crashes_clean['mode_type_clean'].fillna('OTHER')\n",
    "    \n",
    "    # ===== Classification de la gravit√© des accidents =====\n",
    "    print(\"\\n‚öñÔ∏è Classification de la gravit√©...\")\n",
    "    \n",
    "    def classify_crash_severity(mode_type, location_type):\n",
    "        \"\"\"Classifie la gravit√© d'un accident\"\"\"\n",
    "        # Les accidents impliquant pi√©tons et v√©los sont plus graves\n",
    "        if mode_type in ['PEDESTRIAN', 'BICYCLE']:\n",
    "            if location_type == 'Intersection':\n",
    "                return 'HIGH'\n",
    "            else:\n",
    "                return 'MEDIUM'\n",
    "        elif mode_type == 'MOTOR_VEHICLE':\n",
    "            if location_type == 'Intersection':\n",
    "                return 'MEDIUM'\n",
    "            else:\n",
    "                return 'LOW'\n",
    "        else:\n",
    "            return 'MEDIUM'\n",
    "    \n",
    "    if 'mode_type_clean' in df_crashes_clean.columns:\n",
    "        df_crashes_clean['crash_severity'] = df_crashes_clean.apply(\n",
    "            lambda x: classify_crash_severity(\n",
    "                x.get('mode_type_clean'), \n",
    "                x.get('location_type')\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset Crashes nettoy√©: {len(df_crashes_clean):,} lignes\")\n",
    "    \n",
    "    # Statistiques des modes de transport\n",
    "    if 'mode_type_clean' in df_crashes_clean.columns:\n",
    "        mode_counts = df_crashes_clean['mode_type_clean'].value_counts()\n",
    "        print(\"   R√©partition par mode de transport:\")\n",
    "        for mode, count in mode_counts.items():\n",
    "            print(f\"     - {mode}: {count:,} ({count/len(df_crashes_clean)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 5. HARMONISATION DES DATASETS\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó HARMONISATION DES DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===== Standardisation des colonnes temporelles =====\n",
    "print(\"\\n‚è∞ Standardisation des colonnes temporelles...\")\n",
    "\n",
    "# Cr√©er un format uniforme pour tous les datasets\n",
    "def standardize_temporal_columns(df, date_col, prefix):\n",
    "    \"\"\"Standardise les colonnes temporelles avec un pr√©fixe commun\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Colonnes standardis√©es\n",
    "    result_df[f'{prefix}_datetime'] = df[date_col]\n",
    "    result_df[f'{prefix}_year'] = df[date_col].dt.year\n",
    "    result_df[f'{prefix}_month'] = df[date_col].dt.month\n",
    "    result_df[f'{prefix}_day'] = df[date_col].dt.day\n",
    "    result_df[f'{prefix}_weekday'] = df[date_col].dt.dayofweek\n",
    "    result_df[f'{prefix}_hour'] = df[date_col].dt.hour\n",
    "    result_df[f'{prefix}_date'] = df[date_col].dt.date\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Application de la standardisation\n",
    "standardized_datasets = {}\n",
    "\n",
    "if 'df_311_clean' in locals():\n",
    "    standardized_datasets['311'] = standardize_temporal_columns(df_311_clean, 'open_dt', 'incident')\n",
    "    # Ajout de colonnes identificatrices\n",
    "    standardized_datasets['311']['data_source'] = '311_REPORTS'\n",
    "    standardized_datasets['311']['incident_type'] = '311'\n",
    "\n",
    "if 'df_crime_clean' in locals():\n",
    "    standardized_datasets['crime'] = standardize_temporal_columns(df_crime_clean, 'OCCURRED_ON_DATE', 'incident')\n",
    "    standardized_datasets['crime']['data_source'] = 'CRIME_INCIDENTS'\n",
    "    standardized_datasets['crime']['incident_type'] = 'CRIME'\n",
    "\n",
    "if 'df_crashes_clean' in locals():\n",
    "    standardized_datasets['crashes'] = standardize_temporal_columns(df_crashes_clean, 'dispatch_ts', 'incident')\n",
    "    standardized_datasets['crashes']['data_source'] = 'TRAFFIC_CRASHES'\n",
    "    standardized_datasets['crashes']['incident_type'] = 'CRASH'\n",
    "\n",
    "# ===== Standardisation des coordonn√©es =====\n",
    "print(\"\\nüó∫Ô∏è Standardisation des coordonn√©es...\")\n",
    "\n",
    "def standardize_coordinates(df, lat_col, lon_col):\n",
    "    \"\"\"Standardise les colonnes de coordonn√©es\"\"\"\n",
    "    result_df = df.copy()\n",
    "    result_df['latitude'] = df[lat_col]\n",
    "    result_df['longitude'] = df[lon_col]\n",
    "    return result_df\n",
    "\n",
    "# Application aux datasets\n",
    "for name, df in standardized_datasets.items():\n",
    "    if name == '311':\n",
    "        # D√©j√† standardis√©\n",
    "        pass\n",
    "    elif name == 'crime':\n",
    "        standardized_datasets[name] = standardize_coordinates(df, 'Lat', 'Long')\n",
    "    elif name == 'crashes':\n",
    "        standardized_datasets[name] = standardize_coordinates(df, 'lat', 'long')\n",
    "\n",
    "print(\"‚úÖ Coordonn√©es standardis√©es pour tous les datasets\")\n",
    "\n",
    "# ===== Cr√©ation d'un dataset unifi√© minimal =====\n",
    "print(\"\\nüîÑ Cr√©ation d'un dataset unifi√©...\")\n",
    "\n",
    "unified_columns = [\n",
    "    'data_source', 'incident_type', 'incident_datetime', \n",
    "    'latitude', 'longitude', 'incident_year', 'incident_month', \n",
    "    'incident_day', 'incident_weekday', 'incident_hour'\n",
    "]\n",
    "\n",
    "unified_datasets = []\n",
    "\n",
    "for name, df in standardized_datasets.items():\n",
    "    # S√©lection des colonnes communes\n",
    "    available_cols = [col for col in unified_columns if col in df.columns]\n",
    "    df_subset = df[available_cols].copy()\n",
    "    \n",
    "    # Ajout de colonnes sp√©cifiques selon le type\n",
    "    if name == '311':\n",
    "        if 'priority' in df.columns:\n",
    "            df_subset['severity'] = df['priority']\n",
    "        else:\n",
    "            df_subset['severity'] = 'MEDIUM'\n",
    "    elif name == 'crime':\n",
    "        if 'severity' in df.columns:\n",
    "            df_subset['severity'] = df['severity']\n",
    "        else:\n",
    "            df_subset['severity'] = 'HIGH'\n",
    "    elif name == 'crashes':\n",
    "        if 'crash_severity' in df.columns:\n",
    "            df_subset['severity'] = df['crash_severity']\n",
    "        else:\n",
    "            df_subset['severity'] = 'MEDIUM'\n",
    "    \n",
    "    unified_datasets.append(df_subset)\n",
    "\n",
    "# Combinaison des datasets\n",
    "if unified_datasets:\n",
    "    df_unified = pd.concat(unified_datasets, ignore_index=True)\n",
    "    print(f\"‚úÖ Dataset unifi√© cr√©√©: {len(df_unified):,} incidents\")\n",
    "    \n",
    "    # Statistiques du dataset unifi√©\n",
    "    print(\"\\nüìä Statistiques du dataset unifi√©:\")\n",
    "    source_counts = df_unified['data_source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  - {source}: {count:,} ({count/len(df_unified)*100:.1f}%)\")\n",
    "    \n",
    "    severity_counts = df_unified['severity'].value_counts()\n",
    "    print(\"\\n  R√©partition par gravit√© globale:\")\n",
    "    for severity, count in severity_counts.items():\n",
    "        print(f\"    - {severity}: {count:,} ({count/len(df_unified)*100:.1f}%)\")\n",
    "\n",
    "# =====================================================================\n",
    "# 6. VALIDATION DE LA QUALIT√â DES DONN√âES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ VALIDATION DE LA QUALIT√â DES DONN√âES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def validate_dataset_quality(df, dataset_name):\n",
    "    \"\"\"Valide la qualit√© d'un dataset nettoy√©\"\"\"\n",
    "    print(f\"\\nüîç Validation pour {dataset_name}:\")\n",
    "    \n",
    "    # Comptage des valeurs manquantes\n",
    "    missing_counts = df.isnull().sum()\n",
    "    critical_columns = ['latitude', 'longitude', 'incident_datetime']\n",
    "    \n",
    "    for col in critical_columns:\n",
    "        if col in df.columns:\n",
    "            missing_count = missing_counts[col]\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            if missing_pct > 5:\n",
    "                print(f\"  ‚ö†Ô∏è {col}: {missing_count:,} manquantes ({missing_pct:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ {col}: {missing_count:,} manquantes ({missing_pct:.1f}%)\")\n",
    "    \n",
    "    # Validation temporelle\n",
    "    if 'incident_datetime' in df.columns:\n",
    "        # Calcul de la p√©riode couverte\n",
    "        try:\n",
    "            date_range = df['incident_datetime'].max() - df['incident_datetime'].min()\n",
    "            print(f\"  üìÖ P√©riode couverte: {date_range.days} jours\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Erreur calcul p√©riode: {str(e)}\")\n",
    "        \n",
    "        # D√©tection d'anomalies temporelles - version robuste\n",
    "        try:\n",
    "            # Convertir toutes les dates en naive datetime pour comparaison\n",
    "            current_time = pd.Timestamp.now()\n",
    "            \n",
    "            # Si les colonnes ont une timezone, on la supprime\n",
    "            if hasattr(df['incident_datetime'].dtype, 'tz') and df['incident_datetime'].dtype.tz is not None:\n",
    "                incident_dates = df['incident_datetime'].dt.tz_localize(None)\n",
    "            else:\n",
    "                incident_dates = df['incident_datetime']\n",
    "            \n",
    "            # S'assurer que current_time est aussi naive\n",
    "            if hasattr(current_time, 'tz') and current_time.tz is not None:\n",
    "                current_time = current_time.tz_localize(None)\n",
    "            \n",
    "            # Comparaison s√©curis√©e\n",
    "            future_mask = incident_dates > current_time\n",
    "            future_count = future_mask.sum()\n",
    "            \n",
    "            if future_count > 0:\n",
    "                print(f\"  ‚ö†Ô∏è Dates futures d√©tect√©es: {future_count:,}\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ Aucune date future d√©tect√©e\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Impossible de v√©rifier les dates futures: {str(e)}\")\n",
    "    \n",
    "    # Validation g√©ospatiale\n",
    "    if 'latitude' in df.columns and 'longitude' in df.columns:\n",
    "        try:\n",
    "            coord_bounds = {\n",
    "                'lat_min': df['latitude'].min(),\n",
    "                'lat_max': df['latitude'].max(),\n",
    "                'lon_min': df['longitude'].min(),\n",
    "                'lon_max': df['longitude'].max()\n",
    "            }\n",
    "            \n",
    "            # V√©rification des limites de Boston\n",
    "            boston_bounds = {\n",
    "                'lat_min': 42.1, 'lat_max': 42.7,\n",
    "                'lon_min': -71.3, 'lon_max': -70.8\n",
    "            }\n",
    "            \n",
    "            outside_bounds = (\n",
    "                (df['latitude'] < boston_bounds['lat_min']) |\n",
    "                (df['latitude'] > boston_bounds['lat_max']) |\n",
    "                (df['longitude'] < boston_bounds['lon_min']) |\n",
    "                (df['longitude'] > boston_bounds['lon_max'])\n",
    "            ).sum()\n",
    "            \n",
    "            if outside_bounds > 0:\n",
    "                print(f\"  ‚ö†Ô∏è Points hors limites Boston: {outside_bounds:,}\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ Toutes les coordonn√©es dans les limites de Boston\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Erreur validation g√©ospatiale: {str(e)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Validation des datasets nettoy√©s\n",
    "if standardized_datasets:\n",
    "    for name, df in standardized_datasets.items():\n",
    "        validate_dataset_quality(df, name.upper())\n",
    "\n",
    "if 'df_unified' in locals():\n",
    "    validate_dataset_quality(df_unified, \"DATASET UNIFI√â\")\n",
    "\n",
    "# =====================================================================\n",
    "# 7. SAUVEGARDE DES DONN√âES NETTOY√âES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ SAUVEGARDE DES DONN√âES NETTOY√âES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "\n",
    "# Cr√©ation du dossier de sortie\n",
    "output_dir = 'data/processed'\n",
    "os.makedirs(\"../\" + output_dir, exist_ok=True)\n",
    "\n",
    "# Fonction de sauvegarde flexible\n",
    "def save_dataframe(df, filename_base, output_dir):\n",
    "    \"\"\"Sauvegarde un dataframe en essayant plusieurs formats\"\"\"\n",
    "    saved_path = None\n",
    "    \n",
    "    # Essayer Parquet d'abord (plus efficace)\n",
    "    try:\n",
    "        parquet_path = f\"../{output_dir}/{filename_base}.parquet\"\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        saved_path = parquet_path\n",
    "        print(f\"‚úÖ Sauvegard√©: {parquet_path} ({len(df):,} lignes) [Parquet]\")\n",
    "    except ImportError:\n",
    "        # Si Parquet n'est pas disponible, utiliser CSV\n",
    "        try:\n",
    "            csv_path = f\"../{output_dir}/{filename_base}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            saved_path = csv_path\n",
    "            print(f\"‚úÖ Sauvegard√©: {csv_path} ({len(df):,} lignes) [CSV - Parquet non disponible]\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sauvegarde {filename_base}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur sauvegarde Parquet {filename_base}: {e}\")\n",
    "        # Fallback vers CSV\n",
    "        try:\n",
    "            csv_path = f\"../{output_dir}/{filename_base}.csv\"\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            saved_path = csv_path\n",
    "            print(f\"‚úÖ Sauvegard√©: {csv_path} ({len(df):,} lignes) [CSV - Fallback]\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Erreur sauvegarde CSV {filename_base}: {e2}\")\n",
    "    \n",
    "    return saved_path\n",
    "\n",
    "# Sauvegarde des datasets individuels\n",
    "saved_files = []\n",
    "\n",
    "for name, df in standardized_datasets.items():\n",
    "    saved_path = save_dataframe(df, f\"cleaned_{name}\", output_dir)\n",
    "    if saved_path:\n",
    "        saved_files.append(saved_path)\n",
    "\n",
    "# Sauvegarde du dataset unifi√©\n",
    "if 'df_unified' in locals():\n",
    "    saved_path = save_dataframe(df_unified, \"unified_incidents\", output_dir)\n",
    "    if saved_path:\n",
    "        saved_files.append(saved_path)\n",
    "\n",
    "# Message d'installation de pyarrow si n√©cessaire\n",
    "if any('.csv' in path for path in saved_files):\n",
    "    print(\"\\nüí° CONSEIL:\")\n",
    "    print(\"   Pour des performances optimales, installez pyarrow:\")\n",
    "    print(\"   pip install pyarrow\")\n",
    "    print(\"   Cela permettra l'utilisation du format Parquet (plus rapide et compact)\")\n",
    "\n",
    "# Sauvegarde des m√©tadonn√©es\n",
    "def get_temporal_coverage(df):\n",
    "    \"\"\"Calcule la couverture temporelle en g√©rant les timezones\"\"\"\n",
    "    try:\n",
    "        if 'incident_datetime' not in df.columns:\n",
    "            return \"N/A\"\n",
    "        \n",
    "        # Normalisation des timezones pour le calcul\n",
    "        datetime_series = df['incident_datetime'].copy()\n",
    "        \n",
    "        # Conversion robuste des timezones\n",
    "        if pd.api.types.is_datetime64tz_dtype(datetime_series):\n",
    "            # Si c'est un datetime avec timezone, convertir en naive\n",
    "            datetime_series = datetime_series.dt.tz_convert(None)\n",
    "        elif pd.api.types.is_datetime64_dtype(datetime_series):\n",
    "            # Si c'est d√©j√† un datetime naive, pas de changement\n",
    "            pass\n",
    "        else:\n",
    "            # Conversion vers datetime si ce n'est pas d√©j√† le cas\n",
    "            datetime_series = pd.to_datetime(datetime_series)\n",
    "        \n",
    "        # Assurer que c'est bien naive maintenant\n",
    "        if pd.api.types.is_datetime64tz_dtype(datetime_series):\n",
    "            datetime_series = datetime_series.dt.tz_localize(None)\n",
    "        \n",
    "        # Calcul s√©curis√© du min et max\n",
    "        min_date = datetime_series.min()\n",
    "        max_date = datetime_series.max()\n",
    "        \n",
    "        # Conversion en string pour √©viter les probl√®mes de s√©rialisation JSON\n",
    "        return f\"{str(min_date)} to {str(max_date)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "metadata = {\n",
    "    'preprocessing_date': datetime.now().isoformat(),\n",
    "    'datasets_processed': list(standardized_datasets.keys()),\n",
    "    'total_incidents': len(df_unified) if 'df_unified' in locals() else 0,\n",
    "    'data_quality': {\n",
    "        'coordinate_coverage': len(df_unified[df_unified[['latitude', 'longitude']].notna().all(axis=1)]) / len(df_unified) * 100 if 'df_unified' in locals() else 0,\n",
    "        'temporal_coverage': get_temporal_coverage(df_unified) if 'df_unified' in locals() else \"N/A\"\n",
    "    },\n",
    "    'file_formats_used': ['parquet' if any('.parquet' in path for path in saved_files) else 'csv']\n",
    "}\n",
    "\n",
    "metadata_filename = f\"../{output_dir}/preprocessing_metadata.json\"\n",
    "import json\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ M√©tadonn√©es sauvegard√©es: {metadata_filename}\")\n",
    "\n",
    "# =====================================================================\n",
    "# 8. R√âSUM√â DU PREPROCESSING\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã R√âSUM√â DU PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ DATASETS TRAIT√âS:\")\n",
    "if standardized_datasets:\n",
    "    for name, df in standardized_datasets.items():\n",
    "        print(f\"  ‚Ä¢ {name.upper()}: {len(df):,} incidents\")\n",
    "\n",
    "if 'df_unified' in locals():\n",
    "    print(f\"\\nüìä DATASET UNIFI√â: {len(df_unified):,} incidents totaux\")\n",
    "    \n",
    "    # Calcul s√©curis√© de la p√©riode avec gestion de timezone\n",
    "    try:\n",
    "        # Normalisation des timezones pour l'affichage\n",
    "        datetime_series = df_unified['incident_datetime']\n",
    "        if hasattr(datetime_series.dtype, 'tz') and datetime_series.dtype.tz is not None:\n",
    "            datetime_series = datetime_series.dt.tz_localize(None)\n",
    "        \n",
    "        min_date = datetime_series.min()\n",
    "        max_date = datetime_series.max()\n",
    "        print(f\"  ‚Ä¢ P√©riode: {min_date} √† {max_date}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚Ä¢ P√©riode: Erreur calcul dates - {str(e)}\")\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Couverture g√©ographique: {len(df_unified[df_unified[['latitude', 'longitude']].notna().all(axis=1)])} incidents avec coordonn√©es\")\n",
    "\n",
    "print(f\"\\nüíæ FICHIERS SAUVEGARD√âS:\")\n",
    "for filename in saved_files:\n",
    "    print(f\"  ‚Ä¢ {filename}\")\n",
    "\n",
    "print(f\"\\n‚úÖ QUALIT√â DES DONN√âES:\")\n",
    "print(f\"  ‚Ä¢ Coordonn√©es valides: {metadata['data_quality']['coordinate_coverage']:.1f}%\")\n",
    "print(f\"  ‚Ä¢ P√©riode temporelle: {metadata['data_quality']['temporal_coverage']}\")\n",
    "\n",
    "print(\"\\nüöÄ PROCHAINE √âTAPE:\")\n",
    "print(\"  Les donn√©es sont pr√™tes pour le feature engineering!\")\n",
    "print(\"  Prochain notebook: 03_feature_engineering.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ú® PREPROCESSING TERMIN√â AVEC SUCC√àS\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
