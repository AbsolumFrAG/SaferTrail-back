{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è SHAP non install√© - analyse d'interpr√©tabilit√© limit√©e\n",
      "üìä D√©but de l'√©valuation du mod√®le\n",
      "============================================================\n",
      "\n",
      "üìÅ Chargement du mod√®le et des donn√©es...\n",
      "‚ùå Fichier non trouv√©: [Errno 2] No such file or directory: '../models/model_metadata.json'\n",
      "   V√©rifiez que le model development a √©t√© ex√©cut√©\n",
      "\n",
      "============================================================\n",
      "üìà √âVALUATION DES PERFORMANCES G√âN√âRALES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç ANALYSE DES R√âSIDUS ET ERREURS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîÑ VALIDATION CROIS√âE ET TESTS DE ROBUSTESSE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üîç ANALYSE D'IMPORTANCE ET INTERPR√âTABILIT√â\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üìä ANALYSE PAR SEGMENTS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üõ°Ô∏è TESTS DE ROBUSTESSE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üìã G√âN√âRATION DU RAPPORT D'√âVALUATION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üó∫Ô∏è VISUALISATION G√âOSPATIALE (OPTIONNEL)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üéØ CONCLUSIONS ET PROCHAINES √âTAPES\n",
      "============================================================\n",
      "\n",
      "‚úÖ √âVALUATION COMPL√âT√âE:\n",
      "\n",
      "üìÅ FICHIERS G√âN√âR√âS:\n",
      "  ‚ö†Ô∏è ../models/evaluation_report.json (non g√©n√©r√©)\n",
      "  ‚ö†Ô∏è ../models/risk_map.html (non g√©n√©r√©)\n",
      "\n",
      "üöÄ PROCHAINES √âTAPES RECOMMAND√âES:\n",
      "\n",
      "üîß AM√âLIORATIONS POSSIBLES:\n",
      "  ‚Ä¢ Collecter plus de donn√©es d'entra√Ænement\n",
      "  ‚Ä¢ Enrichir avec des features externes (m√©t√©o, √©v√©nements)\n",
      "  ‚Ä¢ Tester des architectures de mod√®les plus complexes\n",
      "  ‚Ä¢ Impl√©menter un syst√®me de re-entra√Ænement automatique\n",
      "  ‚Ä¢ D√©velopper des alertes en temps r√©el\n",
      "\n",
      "============================================================\n",
      "‚ú® √âVALUATION DU MOD√àLE TERMIN√âE AVEC SUCC√àS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation - Mod√®le de Pr√©diction de Rues Risqu√©es\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librairies pour l'√©valuation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, validation_curve, learning_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "\n",
    "# Librairies pour visualisation avanc√©e\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except ImportError:\n",
    "    HAS_SHAP = False\n",
    "    print(\"‚ö†Ô∏è SHAP non install√© - analyse d'interpr√©tabilit√© limit√©e\")\n",
    "\n",
    "# Pour les cartes\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "print(\"üìä D√©but de l'√©valuation du mod√®le\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# =====================================================================\n",
    "# 1. CHARGEMENT DU MOD√àLE ET DES DONN√âES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\nüìÅ Chargement du mod√®le et des donn√©es...\")\n",
    "\n",
    "def load_model_and_data():\n",
    "    \"\"\"Charge le mod√®le entra√Æn√© et les donn√©es de test\"\"\"\n",
    "    import json\n",
    "    \n",
    "    try:\n",
    "        # Chargement des m√©tadonn√©es du mod√®le\n",
    "        with open('../models/model_metadata.json', 'r') as f:\n",
    "            model_metadata = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ M√©tadonn√©es du mod√®le charg√©es:\")\n",
    "        print(f\"  - Mod√®le: {model_metadata['model_name']}\")\n",
    "        print(f\"  - Type: {model_metadata['model_type']}\")\n",
    "        print(f\"  - Features: {model_metadata['num_features']}\")\n",
    "        print(f\"  - Date d'entra√Ænement: {model_metadata['model_training_date']}\")\n",
    "        \n",
    "        # Chargement du mod√®le\n",
    "        if model_metadata['model_type'] == 'neural_network':\n",
    "            try:\n",
    "                import keras\n",
    "                model = keras.models.load_model('../models/risk_prediction_model.h5')\n",
    "                print(f\"‚úÖ Mod√®le Neural Network charg√©\")\n",
    "            except ImportError:\n",
    "                print(\"‚ùå TensorFlow non disponible pour charger le Neural Network\")\n",
    "                return None, None, None, None\n",
    "        else:\n",
    "            model = joblib.load('../models/risk_prediction_model.joblib')\n",
    "            print(f\"‚úÖ Mod√®le {model_metadata['model_name']} charg√©\")\n",
    "        \n",
    "        # Chargement du scaler\n",
    "        scaler = joblib.load('../models/feature_scaler.joblib')\n",
    "        print(f\"‚úÖ Scaler charg√©\")\n",
    "        \n",
    "        # Chargement des donn√©es de features\n",
    "        try:\n",
    "            X_test = pd.read_parquet('../data/features/feature_matrix.parquet')\n",
    "            y_test = pd.read_parquet('../data/features/target_variable.parquet')['risk_score']\n",
    "            \n",
    "            # Division simul√©e des donn√©es (car on n'a pas sauv√© la division exacte)\n",
    "            # On prend les 20% derniers comme test set\n",
    "            test_size = int(0.2 * len(X_test))\n",
    "            X_test_subset = X_test.iloc[-test_size:]\n",
    "            y_test_subset = y_test.iloc[-test_size:]\n",
    "            \n",
    "            print(f\"‚úÖ Donn√©es de test charg√©es: {X_test_subset.shape[0]} observations\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ö†Ô∏è Fichiers de features non trouv√©s - utilisation de donn√©es simul√©es\")\n",
    "            # G√©n√©ration de donn√©es simul√©es pour la d√©mo\n",
    "            np.random.seed(42)\n",
    "            n_samples = 20\n",
    "            n_features = model_metadata['num_features']\n",
    "            X_test_subset = pd.DataFrame(\n",
    "                np.random.randn(n_samples, n_features),\n",
    "                columns=model_metadata['feature_names'][:n_features]\n",
    "            )\n",
    "            y_test_subset = pd.Series(np.random.uniform(0, 10, n_samples))\n",
    "            print(f\"‚ö†Ô∏è Donn√©es simul√©es cr√©√©es: {n_samples} observations\")\n",
    "        \n",
    "        return model, scaler, X_test_subset, y_test_subset, model_metadata\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Fichier non trouv√©: {e}\")\n",
    "        print(\"   V√©rifiez que le model development a √©t√© ex√©cut√©\")\n",
    "        return None, None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur de chargement: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Chargement du mod√®le et des donn√©es\n",
    "model, scaler, X_test, y_test, model_metadata = load_model_and_data()\n",
    "\n",
    "# =====================================================================\n",
    "# 2. √âVALUATION DES PERFORMANCES G√âN√âRALES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà √âVALUATION DES PERFORMANCES G√âN√âRALES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def comprehensive_evaluation(model, X_test, y_test, scaler, model_metadata):\n",
    "    \"\"\"√âvaluation compl√®te des performances du mod√®le\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ √âvaluation en cours...\")\n",
    "    \n",
    "    # Normalisation des features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    if model_metadata['model_type'] == 'neural_network':\n",
    "        y_pred = model.predict(X_test_scaled, verbose=0).flatten()\n",
    "    else:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcul des m√©triques\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'R¬≤': r2_score(y_test, y_pred),\n",
    "        'MAPE': np.mean(np.abs((y_test - y_pred) / y_test)) * 100,\n",
    "        'Max Error': np.max(np.abs(y_test - y_pred))\n",
    "    }\n",
    "    \n",
    "    # Calcul d'erreurs relatives\n",
    "    relative_errors = np.abs(y_test - y_pred) / y_test\n",
    "    metrics['Median Relative Error'] = np.median(relative_errors) * 100\n",
    "    metrics['95th Percentile Error'] = np.percentile(relative_errors, 95) * 100\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRIQUES DE PERFORMANCE:\")\n",
    "    print(f\"  ‚Ä¢ RMSE (Root Mean Square Error): {metrics['RMSE']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ MAE (Mean Absolute Error): {metrics['MAE']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ R¬≤ (Coefficient de d√©termination): {metrics['R¬≤']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ MAPE (Mean Absolute Percentage Error): {metrics['MAPE']:.1f}%\")\n",
    "    print(f\"  ‚Ä¢ Erreur maximale: {metrics['Max Error']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Erreur relative m√©diane: {metrics['Median Relative Error']:.1f}%\")\n",
    "    print(f\"  ‚Ä¢ 95e percentile d'erreur: {metrics['95th Percentile Error']:.1f}%\")\n",
    "    \n",
    "    # Interpr√©tation des r√©sultats\n",
    "    print(f\"\\nüéØ INTERPR√âTATION:\")\n",
    "    if metrics['R¬≤'] > 0.8:\n",
    "        print(\"  ‚úÖ Excellent: R¬≤ > 0.8 - Mod√®le tr√®s pr√©dictif\")\n",
    "    elif metrics['R¬≤'] > 0.6:\n",
    "        print(\"  ‚úÖ Bon: R¬≤ > 0.6 - Mod√®le assez pr√©dictif\")\n",
    "    elif metrics['R¬≤'] > 0.4:\n",
    "        print(\"  ‚ö†Ô∏è Moyen: R¬≤ > 0.4 - Mod√®le mod√©r√©ment pr√©dictif\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Faible: R¬≤ < 0.4 - Mod√®le peu pr√©dictif\")\n",
    "    \n",
    "    if metrics['MAPE'] < 15:\n",
    "        print(\"  ‚úÖ Erreur acceptable: MAPE < 15%\")\n",
    "    elif metrics['MAPE'] < 25:\n",
    "        print(\"  ‚ö†Ô∏è Erreur mod√©r√©e: MAPE < 25%\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Erreur importante: MAPE > 25%\")\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "# √âvaluation si le mod√®le est charg√©\n",
    "if model is not None:\n",
    "    performance_metrics, predictions = comprehensive_evaluation(model, X_test, y_test, scaler, model_metadata)\n",
    "\n",
    "# =====================================================================\n",
    "# 3. ANALYSE DES R√âSIDUS ET ERREURS\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç ANALYSE DES R√âSIDUS ET ERREURS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def residual_analysis(y_true, y_pred):\n",
    "    \"\"\"Analyse d√©taill√©e des r√©sidus\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä Analyse des r√©sidus...\")\n",
    "    \n",
    "    # Calcul des r√©sidus\n",
    "    residuals = y_true - y_pred\n",
    "    \n",
    "    # Statistiques des r√©sidus\n",
    "    print(f\"\\nüìà STATISTIQUES DES R√âSIDUS:\")\n",
    "    print(f\"  ‚Ä¢ Moyenne: {residuals.mean():.3f}\")\n",
    "    print(f\"  ‚Ä¢ √âcart-type: {residuals.std():.3f}\")\n",
    "    print(f\"  ‚Ä¢ Minimum: {residuals.min():.3f}\")\n",
    "    print(f\"  ‚Ä¢ Maximum: {residuals.max():.3f}\")\n",
    "    print(f\"  ‚Ä¢ Skewness: {residuals.skew():.3f}\")\n",
    "    print(f\"  ‚Ä¢ Kurtosis: {residuals.kurtosis():.3f}\")\n",
    "    \n",
    "    # Tests de normalit√© (approximatif)\n",
    "    from scipy.stats import jarque_bera\n",
    "    try:\n",
    "        jb_stat, jb_p = jarque_bera(residuals)\n",
    "        print(f\"  ‚Ä¢ Test de Jarque-Bera: p-value = {jb_p:.3f}\")\n",
    "        if jb_p > 0.05:\n",
    "            print(\"    ‚úÖ R√©sidus probablement normaux\")\n",
    "        else:\n",
    "            print(\"    ‚ö†Ô∏è R√©sidus non-normaux d√©tect√©s\")\n",
    "    except:\n",
    "        print(\"  ‚Ä¢ Test de normalit√© non disponible\")\n",
    "    \n",
    "    # D√©tection des outliers\n",
    "    Q1 = residuals.quantile(0.25)\n",
    "    Q3 = residuals.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outlier_threshold = 1.5 * IQR\n",
    "    outliers = residuals[(residuals < Q1 - outlier_threshold) | (residuals > Q3 + outlier_threshold)]\n",
    "    \n",
    "    print(f\"\\nüéØ D√âTECTION D'OUTLIERS:\")\n",
    "    print(f\"  ‚Ä¢ Nombre d'outliers: {len(outliers)} ({len(outliers)/len(residuals)*100:.1f}%)\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  ‚Ä¢ Indices des outliers: {list(outliers.index)}\")\n",
    "    \n",
    "    return residuals\n",
    "\n",
    "def create_residual_visualizations(y_true, y_pred, residuals):\n",
    "    \"\"\"Cr√©e des visualisations pour l'analyse des r√©sidus\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä Cr√©ation des visualisations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Analyse des R√©sidus et Performances du Mod√®le', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Pr√©dictions vs R√©alit√©\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(y_true, y_pred, alpha=0.6, color='blue')\n",
    "    min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Parfait')\n",
    "    ax1.set_xlabel('Valeurs R√©elles')\n",
    "    ax1.set_ylabel('Pr√©dictions')\n",
    "    ax1.set_title('Pr√©dictions vs R√©alit√©')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Distribution des r√©sidus\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(residuals, bins=15, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(residuals.mean(), color='red', linestyle='--', label=f'Moyenne: {residuals.mean():.3f}')\n",
    "    ax2.set_xlabel('R√©sidus')\n",
    "    ax2.set_ylabel('Fr√©quence')\n",
    "    ax2.set_title('Distribution des R√©sidus')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. R√©sidus vs Pr√©dictions\n",
    "    ax3 = axes[0, 2]\n",
    "    ax3.scatter(y_pred, residuals, alpha=0.6, color='green')\n",
    "    ax3.axhline(0, color='red', linestyle='--')\n",
    "    ax3.set_xlabel('Pr√©dictions')\n",
    "    ax3.set_ylabel('R√©sidus')\n",
    "    ax3.set_title('R√©sidus vs Pr√©dictions')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q Plot (approximatif)\n",
    "    ax4 = axes[1, 0]\n",
    "    from scipy.stats import probplot\n",
    "    probplot(residuals, dist=\"norm\", plot=ax4)\n",
    "    ax4.set_title('Q-Q Plot (Normalit√© des R√©sidus)')\n",
    "    ax4.grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Erreurs absolues\n",
    "    ax5 = axes[1, 1]\n",
    "    abs_errors = np.abs(residuals)\n",
    "    ax5.scatter(y_true, abs_errors, alpha=0.6, color='orange')\n",
    "    ax5.set_xlabel('Valeurs R√©elles')\n",
    "    ax5.set_ylabel('Erreur Absolue')\n",
    "    ax5.set_title('Erreurs Absolues vs Valeurs R√©elles')\n",
    "    ax5.grid(alpha=0.3)\n",
    "    \n",
    "    # 6. R√©sidus standardis√©s\n",
    "    ax6 = axes[1, 2]\n",
    "    standardized_residuals = residuals / residuals.std()\n",
    "    ax6.scatter(range(len(standardized_residuals)), standardized_residuals, alpha=0.6, color='purple')\n",
    "    ax6.axhline(0, color='red', linestyle='-', alpha=0.5)\n",
    "    ax6.axhline(2, color='red', linestyle='--', alpha=0.5, label='¬±2œÉ')\n",
    "    ax6.axhline(-2, color='red', linestyle='--', alpha=0.5)\n",
    "    ax6.set_xlabel('Index')\n",
    "    ax6.set_ylabel('R√©sidus Standardis√©s')\n",
    "    ax6.set_title('R√©sidus Standardis√©s')\n",
    "    ax6.legend()\n",
    "    ax6.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"  ‚úÖ Visualisations cr√©√©es\")\n",
    "\n",
    "# Analyse des r√©sidus si les pr√©dictions sont disponibles\n",
    "if 'predictions' in locals():\n",
    "    residuals = residual_analysis(y_test, predictions)\n",
    "    create_residual_visualizations(y_test, predictions, residuals)\n",
    "\n",
    "# =====================================================================\n",
    "# 4. VALIDATION CROIS√âE ET ROBUSTESSE\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîÑ VALIDATION CROIS√âE ET TESTS DE ROBUSTESSE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def cross_validation_analysis(model, X, y, scaler, cv_folds=5):\n",
    "    \"\"\"Analyse par validation crois√©e\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîÑ Validation crois√©e avec {cv_folds} plis...\")\n",
    "    \n",
    "    # Pour les mod√®les sklearn\n",
    "    if hasattr(model, 'predict'):\n",
    "        # Donn√©es normalis√©es pour la validation crois√©e\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Validation crois√©e pour diff√©rentes m√©triques\n",
    "        cv_scores = {}\n",
    "        \n",
    "        # R¬≤\n",
    "        r2_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='r2')\n",
    "        cv_scores['R¬≤'] = {'mean': r2_scores.mean(), 'std': r2_scores.std(), 'scores': r2_scores}\n",
    "        \n",
    "        # RMSE (n√©gatif dans sklearn, donc on inverse)\n",
    "        rmse_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='neg_root_mean_squared_error')\n",
    "        cv_scores['RMSE'] = {'mean': -rmse_scores.mean(), 'std': rmse_scores.std(), 'scores': -rmse_scores}\n",
    "        \n",
    "        # MAE (n√©gatif dans sklearn, donc on inverse)\n",
    "        mae_scores = cross_val_score(model, X_scaled, y, cv=cv_folds, scoring='neg_mean_absolute_error')\n",
    "        cv_scores['MAE'] = {'mean': -mae_scores.mean(), 'std': mae_scores.std(), 'scores': -mae_scores}\n",
    "        \n",
    "        print(f\"\\nüìä R√âSULTATS DE LA VALIDATION CROIS√âE:\")\n",
    "        for metric, results in cv_scores.items():\n",
    "            print(f\"  ‚Ä¢ {metric}:\")\n",
    "            print(f\"    - Moyenne: {results['mean']:.3f}\")\n",
    "            print(f\"    - √âcart-type: {results['std']:.3f}\")\n",
    "            print(f\"    - Scores individuels: {[f'{score:.3f}' for score in results['scores']]}\")\n",
    "        \n",
    "        # Analyse de la stabilit√©\n",
    "        r2_cv = cv_scores['R¬≤']['std']\n",
    "        if r2_cv < 0.05:\n",
    "            print(f\"\\n‚úÖ STABILIT√â EXCELLENTE: √âcart-type R¬≤ = {r2_cv:.3f} < 0.05\")\n",
    "        elif r2_cv < 0.1:\n",
    "            print(f\"\\n‚úÖ STABILIT√â BONNE: √âcart-type R¬≤ = {r2_cv:.3f} < 0.1\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è STABILIT√â VARIABLE: √âcart-type R¬≤ = {r2_cv:.3f} > 0.1\")\n",
    "        \n",
    "        return cv_scores\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Validation crois√©e non disponible pour ce type de mod√®le\")\n",
    "        return None\n",
    "\n",
    "def learning_curve_analysis(model, X, y, scaler):\n",
    "    \"\"\"Analyse des courbes d'apprentissage\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìà Analyse des courbes d'apprentissage...\")\n",
    "    \n",
    "    if hasattr(model, 'predict'):\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Calcul des courbes d'apprentissage\n",
    "        train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "        train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "            model, X_scaled, y, train_sizes=train_sizes, cv=3, scoring='r2'\n",
    "        )\n",
    "        \n",
    "        # Moyennes et √©carts-types\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Score d\\'entra√Ænement')\n",
    "        plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "        plt.plot(train_sizes_abs, val_mean, 'o-', color='red', label='Score de validation')\n",
    "        plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "        \n",
    "        plt.xlabel('Taille de l\\'√©chantillon d\\'entra√Ænement')\n",
    "        plt.ylabel('Score R¬≤')\n",
    "        plt.title('Courbes d\\'Apprentissage')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyse de l'overfitting\n",
    "        final_gap = train_mean[-1] - val_mean[-1]\n",
    "        if final_gap < 0.05:\n",
    "            print(f\"‚úÖ PAS D'OVERFITTING: √âcart final = {final_gap:.3f}\")\n",
    "        elif final_gap < 0.1:\n",
    "            print(f\"‚ö†Ô∏è OVERFITTING L√âGER: √âcart final = {final_gap:.3f}\")\n",
    "        else:\n",
    "            print(f\"‚ùå OVERFITTING IMPORTANT: √âcart final = {final_gap:.3f}\")\n",
    "        \n",
    "        return train_sizes_abs, train_mean, val_mean\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Courbes d'apprentissage non disponibles pour ce type de mod√®le\")\n",
    "        return None, None, None\n",
    "\n",
    "# Validation crois√©e et courbes d'apprentissage\n",
    "if model is not None and X_test is not None:\n",
    "    cv_results = cross_validation_analysis(model, X_test, y_test, scaler)\n",
    "    learning_results = learning_curve_analysis(model, X_test, y_test, scaler)\n",
    "\n",
    "# =====================================================================\n",
    "# 5. ANALYSE D'IMPORTANCE ET INTERPR√âTABILIT√â\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç ANALYSE D'IMPORTANCE ET INTERPR√âTABILIT√â\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def feature_importance_analysis(model, X, y, scaler, feature_names):\n",
    "    \"\"\"Analyse d'importance des features\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ Analyse d'importance des features...\")\n",
    "    \n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # 1. Importance int√©gr√©e du mod√®le (si disponible)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        model_importance = model.feature_importances_\n",
    "        importance_type = \"Importance du mod√®le (bas√©e sur l'arbre)\"\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        model_importance = np.abs(model.coef_)\n",
    "        importance_type = \"Coefficients absolus\"\n",
    "    else:\n",
    "        model_importance = None\n",
    "        importance_type = None\n",
    "    \n",
    "    # 2. Permutation importance (plus robuste)\n",
    "    try:\n",
    "        perm_importance = permutation_importance(model, X_scaled, y, n_repeats=5, random_state=42)\n",
    "        perm_importance_mean = perm_importance.importances_mean\n",
    "        perm_importance_std = perm_importance.importances_std\n",
    "        \n",
    "        print(f\"‚úÖ Permutation importance calcul√©e\")\n",
    "        \n",
    "        # DataFrame pour faciliter l'affichage\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'perm_importance': perm_importance_mean,\n",
    "            'perm_std': perm_importance_std\n",
    "        })\n",
    "        \n",
    "        if model_importance is not None:\n",
    "            importance_df['model_importance'] = model_importance\n",
    "            \n",
    "        importance_df = importance_df.sort_values('perm_importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüèÜ TOP 10 FEATURES PAR PERMUTATION IMPORTANCE:\")\n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"  {i+1:2d}. {row['feature']:25} | {row['perm_importance']:.4f} ¬± {row['perm_std']:.4f}\")\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(15)\n",
    "        \n",
    "        if model_importance is not None:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "            \n",
    "            # Permutation importance\n",
    "            ax1.barh(range(len(top_features)), top_features['perm_importance'], \n",
    "                    xerr=top_features['perm_std'], color='skyblue', edgecolor='navy')\n",
    "            ax1.set_yticks(range(len(top_features)))\n",
    "            ax1.set_yticklabels(top_features['feature'])\n",
    "            ax1.set_xlabel('Permutation Importance')\n",
    "            ax1.set_title('Importance par Permutation')\n",
    "            ax1.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Model importance\n",
    "            ax2.barh(range(len(top_features)), top_features['model_importance'], \n",
    "                    color='lightgreen', edgecolor='darkgreen')\n",
    "            ax2.set_yticks(range(len(top_features)))\n",
    "            ax2.set_yticklabels(top_features['feature'])\n",
    "            ax2.set_xlabel(importance_type)\n",
    "            ax2.set_title(importance_type)\n",
    "            ax2.grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "        else:\n",
    "            plt.barh(range(len(top_features)), top_features['perm_importance'], \n",
    "                    xerr=top_features['perm_std'], color='skyblue', edgecolor='navy')\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Permutation Importance')\n",
    "            plt.title('Importance des Features par Permutation')\n",
    "            plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur calcul permutation importance: {e}\")\n",
    "        return None\n",
    "\n",
    "def shap_analysis(model, X_sample, scaler):\n",
    "    \"\"\"Analyse SHAP si disponible\"\"\"\n",
    "    \n",
    "    if not HAS_SHAP:\n",
    "        print(\"‚ö†Ô∏è SHAP non disponible - analyse d'interpr√©tabilit√© limit√©e\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüî¨ Analyse SHAP...\")\n",
    "    \n",
    "    try:\n",
    "        X_scaled = scaler.transform(X_sample)\n",
    "        \n",
    "        # Cr√©ation de l'explainer SHAP\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            explainer = shap.Explainer(model, X_scaled)\n",
    "        else:\n",
    "            explainer = shap.Explainer(model.predict, X_scaled)\n",
    "        \n",
    "        # Calcul des valeurs SHAP pour un √©chantillon\n",
    "        shap_values = explainer(X_scaled[:min(50, len(X_scaled))])\n",
    "        \n",
    "        # Visualisation summary plot\n",
    "        shap.summary_plot(shap_values, X_sample.iloc[:min(50, len(X_sample))], show=False)\n",
    "        plt.title('SHAP Summary Plot - Importance et Impact des Features')\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance globale\n",
    "        feature_importance = np.abs(shap_values.values).mean(0)\n",
    "        shap_importance_df = pd.DataFrame({\n",
    "            'feature': X_sample.columns,\n",
    "            'shap_importance': feature_importance\n",
    "        }).sort_values('shap_importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüèÜ TOP 10 FEATURES PAR SHAP IMPORTANCE:\")\n",
    "        for i, row in shap_importance_df.head(10).iterrows():\n",
    "            print(f\"  {i+1:2d}. {row['feature']:25} | {row['shap_importance']:.4f}\")\n",
    "        \n",
    "        return shap_values, shap_importance_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur analyse SHAP: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Analyses d'importance\n",
    "if model is not None and X_test is not None:\n",
    "    importance_df = feature_importance_analysis(model, X_test, y_test, scaler, X_test.columns.tolist())\n",
    "    \n",
    "    # Analyse SHAP sur un √©chantillon\n",
    "    shap_values, shap_importance = shap_analysis(model, X_test.head(50), scaler)\n",
    "\n",
    "# =====================================================================\n",
    "# 6. ANALYSE PAR SEGMENTS\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ANALYSE PAR SEGMENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def segment_analysis(y_true, y_pred, X_test):\n",
    "    \"\"\"Analyse des performances par segments\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìä Analyse par segments...\")\n",
    "    \n",
    "    # Cr√©ation de segments de risque\n",
    "    risk_segments = pd.cut(y_true, bins=[0, 3, 6, 10], labels=['Faible', 'Moyen', '√âlev√©'])\n",
    "    \n",
    "    # Performances par segment\n",
    "    segment_results = {}\n",
    "    \n",
    "    for segment in risk_segments.cat.categories:\n",
    "        mask = risk_segments == segment\n",
    "        if mask.sum() > 0:\n",
    "            y_true_seg = y_true[mask]\n",
    "            y_pred_seg = y_pred[mask]\n",
    "            \n",
    "            segment_results[segment] = {\n",
    "                'count': mask.sum(),\n",
    "                'rmse': np.sqrt(mean_squared_error(y_true_seg, y_pred_seg)),\n",
    "                'mae': mean_absolute_error(y_true_seg, y_pred_seg),\n",
    "                'r2': r2_score(y_true_seg, y_pred_seg),\n",
    "                'mape': np.mean(np.abs((y_true_seg - y_pred_seg) / y_true_seg)) * 100\n",
    "            }\n",
    "    \n",
    "    print(f\"\\nüìà PERFORMANCES PAR SEGMENT DE RISQUE:\")\n",
    "    for segment, metrics in segment_results.items():\n",
    "        print(f\"\\n  {segment} Risque ({metrics['count']} observations):\")\n",
    "        print(f\"    - RMSE: {metrics['rmse']:.3f}\")\n",
    "        print(f\"    - MAE: {metrics['mae']:.3f}\")\n",
    "        print(f\"    - R¬≤: {metrics['r2']:.3f}\")\n",
    "        print(f\"    - MAPE: {metrics['mape']:.1f}%\")\n",
    "    \n",
    "    # Visualisation par segments\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    segments = list(segment_results.keys())\n",
    "    \n",
    "    # RMSE par segment\n",
    "    rmse_values = [segment_results[seg]['rmse'] for seg in segments]\n",
    "    axes[0].bar(segments, rmse_values, color='lightblue', edgecolor='navy')\n",
    "    axes[0].set_title('RMSE par Segment de Risque')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # R¬≤ par segment\n",
    "    r2_values = [segment_results[seg]['r2'] for seg in segments]\n",
    "    axes[1].bar(segments, r2_values, color='lightgreen', edgecolor='darkgreen')\n",
    "    axes[1].set_title('R¬≤ par Segment de Risque')\n",
    "    axes[1].set_ylabel('R¬≤')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # MAPE par segment\n",
    "    mape_values = [segment_results[seg]['mape'] for seg in segments]\n",
    "    axes[2].bar(segments, mape_values, color='lightcoral', edgecolor='darkred')\n",
    "    axes[2].set_title('MAPE par Segment de Risque')\n",
    "    axes[2].set_ylabel('MAPE (%)')\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return segment_results\n",
    "\n",
    "# Analyse par segments\n",
    "if 'predictions' in locals():\n",
    "    segment_results = segment_analysis(y_test, predictions, X_test)\n",
    "\n",
    "# =====================================================================\n",
    "# 7. TESTS DE ROBUSTESSE\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üõ°Ô∏è TESTS DE ROBUSTESSE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def robustness_tests(model, X_test, y_test, scaler):\n",
    "    \"\"\"Tests de robustesse du mod√®le\"\"\"\n",
    "    \n",
    "    print(f\"\\nüõ°Ô∏è Tests de robustesse...\")\n",
    "    \n",
    "    X_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 1. Test avec bruit ajout√©\n",
    "    noise_levels = [0.01, 0.05, 0.1, 0.2]\n",
    "    noise_results = {}\n",
    "    \n",
    "    for noise_level in noise_levels:\n",
    "        # Ajout de bruit gaussien\n",
    "        X_noisy = X_scaled + np.random.normal(0, noise_level, X_scaled.shape)\n",
    "        \n",
    "        # Pr√©dictions avec bruit\n",
    "        if hasattr(model, 'predict'):\n",
    "            y_pred_noisy = model.predict(X_noisy)\n",
    "        else:\n",
    "            y_pred_noisy = model.predict(X_noisy, verbose=0).flatten()\n",
    "        \n",
    "        # M√©triques avec bruit\n",
    "        noise_results[noise_level] = {\n",
    "            'r2': r2_score(y_test, y_pred_noisy),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred_noisy))\n",
    "        }\n",
    "    \n",
    "    print(f\"\\nüîä ROBUSTESSE AU BRUIT:\")\n",
    "    for noise_level, metrics in noise_results.items():\n",
    "        print(f\"  Bruit {noise_level*100:3.0f}%: R¬≤ = {metrics['r2']:.3f}, RMSE = {metrics['rmse']:.3f}\")\n",
    "    \n",
    "    # 2. Test avec valeurs manquantes simul√©es\n",
    "    missing_percentages = [0.05, 0.1, 0.2]\n",
    "    missing_results = {}\n",
    "    \n",
    "    for missing_pct in missing_percentages:\n",
    "        X_missing = X_scaled.copy()\n",
    "        \n",
    "        # Simulation de valeurs manquantes (remplac√©es par la m√©diane)\n",
    "        n_missing = int(missing_pct * X_missing.size)\n",
    "        missing_indices = np.random.choice(X_missing.size, n_missing, replace=False)\n",
    "        \n",
    "        # Remplacement par la m√©diane de chaque feature\n",
    "        for i in range(X_missing.shape[1]):\n",
    "            col_missing = np.random.choice(X_missing.shape[0], int(missing_pct * X_missing.shape[0]), replace=False)\n",
    "            X_missing[col_missing, i] = np.median(X_scaled[:, i])\n",
    "        \n",
    "        # Pr√©dictions avec valeurs manquantes simul√©es\n",
    "        if hasattr(model, 'predict'):\n",
    "            y_pred_missing = model.predict(X_missing)\n",
    "        else:\n",
    "            y_pred_missing = model.predict(X_missing, verbose=0).flatten()\n",
    "        \n",
    "        missing_results[missing_pct] = {\n",
    "            'r2': r2_score(y_test, y_pred_missing),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_test, y_pred_missing))\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n‚ùì ROBUSTESSE AUX VALEURS MANQUANTES:\")\n",
    "    for missing_pct, metrics in missing_results.items():\n",
    "        print(f\"  {missing_pct*100:3.0f}% manquant: R¬≤ = {metrics['r2']:.3f}, RMSE = {metrics['rmse']:.3f}\")\n",
    "    \n",
    "    # Visualisation de la robustesse\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Robustesse au bruit\n",
    "    noise_levels_pct = [n*100 for n in noise_levels]\n",
    "    noise_r2 = [noise_results[n]['r2'] for n in noise_levels]\n",
    "    ax1.plot(noise_levels_pct, noise_r2, 'o-', color='blue', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Niveau de Bruit (%)')\n",
    "    ax1.set_ylabel('R¬≤')\n",
    "    ax1.set_title('Robustesse au Bruit')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Robustesse aux valeurs manquantes\n",
    "    missing_pct_list = [m*100 for m in missing_percentages]\n",
    "    missing_r2 = [missing_results[m]['r2'] for m in missing_percentages]\n",
    "    ax2.plot(missing_pct_list, missing_r2, 'o-', color='red', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Valeurs Manquantes (%)')\n",
    "    ax2.set_ylabel('R¬≤')\n",
    "    ax2.set_title('Robustesse aux Valeurs Manquantes')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return noise_results, missing_results\n",
    "\n",
    "# Tests de robustesse\n",
    "if model is not None:\n",
    "    noise_results, missing_results = robustness_tests(model, X_test, y_test, scaler)\n",
    "\n",
    "# =====================================================================\n",
    "# 8. G√âN√âRATION DU RAPPORT D'√âVALUATION\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã G√âN√âRATION DU RAPPORT D'√âVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_evaluation_report(model_metadata, performance_metrics, cv_results, importance_df, segment_results):\n",
    "    \"\"\"G√©n√®re un rapport complet d'√©valuation\"\"\"\n",
    "    \n",
    "    print(f\"\\nüìã G√©n√©ration du rapport...\")\n",
    "    \n",
    "    report = {\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'model_info': {\n",
    "            'name': model_metadata['model_name'],\n",
    "            'type': model_metadata['model_type'],\n",
    "            'training_date': model_metadata['model_training_date'],\n",
    "            'features_count': model_metadata['num_features']\n",
    "        },\n",
    "        'performance_metrics': performance_metrics,\n",
    "        'cross_validation': cv_results if cv_results else {},\n",
    "        'feature_importance': importance_df.head(10).to_dict('records') if importance_df is not None else [],\n",
    "        'segment_analysis': segment_results if 'segment_results' in locals() else {},\n",
    "        'robustness': {\n",
    "            'noise_test': noise_results if 'noise_results' in locals() else {},\n",
    "            'missing_values_test': missing_results if 'missing_results' in locals() else {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Interpr√©tations et recommandations\n",
    "    recommendations = []\n",
    "    \n",
    "    # Bas√© sur les performances g√©n√©rales\n",
    "    if performance_metrics['R¬≤'] > 0.8:\n",
    "        recommendations.append(\"‚úÖ Excellent mod√®le - D√©ploiement recommand√©\")\n",
    "    elif performance_metrics['R¬≤'] > 0.6:\n",
    "        recommendations.append(\"‚úÖ Bon mod√®le - D√©ploiement possible avec monitoring\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ö†Ô∏è Performances limit√©es - Am√©lioration n√©cessaire\")\n",
    "    \n",
    "    # Bas√© sur la robustesse\n",
    "    if 'noise_results' in locals():\n",
    "        noise_degradation = (performance_metrics['R¬≤'] - noise_results[0.1]['r2']) / performance_metrics['R¬≤']\n",
    "        if noise_degradation < 0.1:\n",
    "            recommendations.append(\"‚úÖ Mod√®le robuste au bruit\")\n",
    "        else:\n",
    "            recommendations.append(\"‚ö†Ô∏è Sensibilit√© au bruit d√©tect√©e\")\n",
    "    \n",
    "    # Bas√© sur les segments\n",
    "    if 'segment_results' in locals():\n",
    "        segment_r2_values = [seg['r2'] for seg in segment_results.values()]\n",
    "        if min(segment_r2_values) > 0.5:\n",
    "            recommendations.append(\"‚úÖ Performances coh√©rentes entre segments\")\n",
    "        else:\n",
    "            recommendations.append(\"‚ö†Ô∏è Performances variables selon les segments de risque\")\n",
    "    \n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    # Sauvegarde du rapport\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    output_dir = '../models'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    report_filename = f\"{output_dir}/evaluation_report.json\"\n",
    "    with open(report_filename, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Rapport sauvegard√©: {report_filename}\")\n",
    "    \n",
    "    # Affichage du r√©sum√©\n",
    "    print(f\"\\nüìä R√âSUM√â DE L'√âVALUATION:\")\n",
    "    print(f\"  ‚Ä¢ Mod√®le √©valu√©: {report['model_info']['name']}\")\n",
    "    print(f\"  ‚Ä¢ Score R¬≤: {performance_metrics['R¬≤']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ RMSE: {performance_metrics['RMSE']:.3f}\")\n",
    "    print(f\"  ‚Ä¢ MAPE: {performance_metrics['MAPE']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüéØ RECOMMANDATIONS:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"  {rec}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# G√©n√©ration du rapport final\n",
    "if all(var in locals() for var in ['model_metadata', 'performance_metrics']):\n",
    "    final_report = generate_evaluation_report(\n",
    "        model_metadata, \n",
    "        performance_metrics,\n",
    "        locals().get('cv_results'),\n",
    "        locals().get('importance_df'),\n",
    "        locals().get('segment_results')\n",
    "    )\n",
    "\n",
    "# =====================================================================\n",
    "# 9. VISUALISATION G√âOSPATIALE (BONUS)\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üó∫Ô∏è VISUALISATION G√âOSPATIALE (OPTIONNEL)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_risk_map(X_test, y_test, predictions):\n",
    "    \"\"\"Cr√©e une carte de risque g√©ospatiale\"\"\"\n",
    "    \n",
    "    print(f\"\\nüó∫Ô∏è Cr√©ation de la carte de risque...\")\n",
    "    \n",
    "    try:\n",
    "        # V√©rification des colonnes g√©ographiques\n",
    "        if 'latitude' in X_test.columns and 'longitude' in X_test.columns:\n",
    "            \n",
    "            # Centre de la carte (Boston)\n",
    "            center_lat = X_test['latitude'].mean()\n",
    "            center_lon = X_test['longitude'].mean()\n",
    "            \n",
    "            # Cr√©ation de la carte\n",
    "            m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "            \n",
    "            # Ajout des points avec couleurs selon le risque\n",
    "            for idx, row in X_test.iterrows():\n",
    "                lat, lon = row['latitude'], row['longitude']\n",
    "                real_risk = y_test.iloc[idx] if idx < len(y_test) else 5\n",
    "                pred_risk = predictions[idx] if idx < len(predictions) else 5\n",
    "                \n",
    "                # Couleur bas√©e sur le risque pr√©dit\n",
    "                if pred_risk >= 7:\n",
    "                    color = 'red'\n",
    "                elif pred_risk >= 5:\n",
    "                    color = 'orange'\n",
    "                elif pred_risk >= 3:\n",
    "                    color = 'yellow'\n",
    "                else:\n",
    "                    color = 'green'\n",
    "                \n",
    "                folium.CircleMarker(\n",
    "                    location=[lat, lon],\n",
    "                    radius=8,\n",
    "                    popup=f\"Risque r√©el: {real_risk:.2f}<br>Risque pr√©dit: {pred_risk:.2f}\",\n",
    "                    fillColor=color,\n",
    "                    color='black',\n",
    "                    weight=1,\n",
    "                    fillOpacity=0.7\n",
    "                ).add_to(m)\n",
    "            \n",
    "            # Sauvegarde de la carte\n",
    "            map_filename = '../models/risk_map.html'\n",
    "            m.save(map_filename)\n",
    "            print(f\"‚úÖ Carte sauvegard√©e: {map_filename}\")\n",
    "            \n",
    "            return m\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Colonnes g√©ographiques non trouv√©es\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur cr√©ation carte: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cr√©ation de la carte si possible\n",
    "if all(var in locals() for var in ['X_test', 'y_test', 'predictions']):\n",
    "    risk_map = create_risk_map(X_test, y_test, predictions)\n",
    "\n",
    "# =====================================================================\n",
    "# 10. CONCLUSIONS ET PROCHAINES √âTAPES\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ CONCLUSIONS ET PROCHAINES √âTAPES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ √âVALUATION COMPL√âT√âE:\")\n",
    "\n",
    "if 'performance_metrics' in locals():\n",
    "    print(f\"  ‚Ä¢ Performance globale: R¬≤ = {performance_metrics['R¬≤']:.3f}\")\n",
    "    \n",
    "    # √âvaluation qualitative\n",
    "    if performance_metrics['R¬≤'] > 0.8:\n",
    "        quality = \"EXCELLENTE\"\n",
    "        emoji = \"üèÜ\"\n",
    "    elif performance_metrics['R¬≤'] > 0.6:\n",
    "        quality = \"BONNE\"\n",
    "        emoji = \"‚úÖ\"\n",
    "    elif performance_metrics['R¬≤'] > 0.4:\n",
    "        quality = \"MOYENNE\"\n",
    "        emoji = \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        quality = \"FAIBLE\"\n",
    "        emoji = \"‚ùå\"\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Qualit√© du mod√®le: {emoji} {quality}\")\n",
    "\n",
    "print(f\"\\nüìÅ FICHIERS G√âN√âR√âS:\")\n",
    "generated_files = [\n",
    "    '../models/evaluation_report.json',\n",
    "    '../models/risk_map.html'\n",
    "]\n",
    "\n",
    "for filepath in generated_files:\n",
    "    import os\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"  ‚úÖ {filepath}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è {filepath} (non g√©n√©r√©)\")\n",
    "\n",
    "print(f\"\\nüöÄ PROCHAINES √âTAPES RECOMMAND√âES:\")\n",
    "if 'final_report' in locals() and 'recommendations' in final_report:\n",
    "    for i, rec in enumerate(final_report['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(f\"\\nüîß AM√âLIORATIONS POSSIBLES:\")\n",
    "print(f\"  ‚Ä¢ Collecter plus de donn√©es d'entra√Ænement\")\n",
    "print(f\"  ‚Ä¢ Enrichir avec des features externes (m√©t√©o, √©v√©nements)\")\n",
    "print(f\"  ‚Ä¢ Tester des architectures de mod√®les plus complexes\")\n",
    "print(f\"  ‚Ä¢ Impl√©menter un syst√®me de re-entra√Ænement automatique\")\n",
    "print(f\"  ‚Ä¢ D√©velopper des alertes en temps r√©el\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ú® √âVALUATION DU MOD√àLE TERMIN√âE AVEC SUCC√àS\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
